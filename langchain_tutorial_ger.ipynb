{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e9004f10",
      "metadata": {
        "tags": [],
        "id": "e9004f10"
      },
      "source": [
        "![langchain](https://miro.medium.com/v2/resize:fit:853/1*1DBe4cCQYfpM0oNXl_kH2w.png)\n",
        "\n",
        "# LLM-basierte Anwendungen\n",
        "\n",
        "**Autor:** Keno Teppris\n",
        "\n",
        "The following Tutorial showcases the usage of LangChain with german examples. Its a Tutorial that is meant to help participans of a Hackathon to ge started, therefore explanations are in german as well as the Prompts etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8710600c-2e58-4399-bd3d-bb391c2d8c5f",
      "metadata": {
        "tags": [],
        "id": "8710600c-2e58-4399-bd3d-bb391c2d8c5f",
        "outputId": "a544ef5e-95b0-4366-c50b-83316529b9a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pymupdf\n",
            "  Downloading PyMuPDF-1.23.5-cp310-none-manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting PyMuPDFb==1.23.5\n",
            "  Downloading PyMuPDFb-1.23.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyMuPDFb, pymupdf\n",
            "Successfully installed PyMuPDFb-1.23.5 pymupdf-1.23.5\n"
          ]
        }
      ],
      "source": [
        "!pip install pymupdf langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aaabcb77",
      "metadata": {
        "id": "aaabcb77"
      },
      "source": [
        "# Langchain text-generation API wrapper laden\n",
        "\n",
        "Zuerst muss die URL des [text-generation-inference](https://huggingface.co/docs/text-generation-inference/index) Endpoints festlegt werden, damit wir diese URL benutzen, um es als Langchain `LLM` zu laden. Dabei können Parameter eingestellt werden, für welche es sich immer lohnt einmal in die [Dokumentation](https://api.python.langchain.com/en/latest/llms/langchain.llms.huggingface_text_gen_inference.HuggingFaceTextGenInference.html) zu schauen. Hier nur einmal eine Übersicht über die wichtigsten Parameter mit denen man rumspielen kann um den Output zu optimieren:\n",
        "\n",
        "\n",
        "**1. `temperature`:**\n",
        "Stell dir vor, du hast einen Topf mit Wasser. Die \"Temperatur\" entscheidet, ob das Wasser still vor sich hin köchelt oder wild brodelt. In der Sprachmodellierung bestimmt die Temperatur, wie \"kreativ\" oder \"vorhersehbar\" das Modell ist. Hohe Werte machen das Modell mutiger, während niedrigere Werte es zurückhaltender und sicherer machen.\n",
        "\n",
        "**2. `top_k`:**\n",
        "Du bist in einer Eisdiele mit 100 Sorten. Bei `top_k = 5` zeigt dir das Modell nur die 5 wahrscheinlichsten Eissorten. Kein Pistazie für dich, wenn es nicht in den Top-5 ist!\n",
        "\n",
        "**3. `top_p`:**\n",
        "Stell dir vor, jedes Wort, das das Modell als nächstes schreiben könnte, hat eine Wahrscheinlichkeits-Prognose. Bei `top_p = 0,95` wählt das Modell Wörter aus, bis die kombinierten Wahrscheinlichkeiten dieser Wörter 95% erreichen. Es ist wie in einer Eisdiele, wo du sagst: \"Gib mir die Eissorten, die zusammengenommen 95% der Beliebtheit ausmachen.\" Das könnten 2 Sorten sein oder 20 – je nachdem, wie oft sie von Kunden gewählt werden.\n",
        "\n",
        "**4. `max_new_tokens`:**\n",
        "Das ist wie eine Zeilenbegrenzung in deinem Notizbuch. Egal, wie spannend die Geschichte ist, das Modell kann nur so viele Worte (Tokens) schreiben, wie du erlaubst. Es ist, als ob du sagst: \"Erzähl mir was, aber bitte nicht länger als 512 Wörter!\"\n",
        "\n",
        "PS: Um korrekt zu sein, `top_k` und `top_p` beziehen sich nicht auf die Wörter sondern auf Token."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Das folgendene Tutorial wurde für einen Hackathon erstellt und verwendet ein Huggingface Textgeneration Inference Endpoint, welcher nicht öffentlich verfügbar ist. Alternativ kann dieses Tutorial auch durchgeführt werden, indem das jeweiligen Modell lokal in beispielsweise Colab geladen wird. Es wurden ursprünglich folgende Modelle verwendet:\n",
        "\n",
        "1. [em_german_mistral_v01-AWQ](https://huggingface.co/TheBloke/em_german_mistral_v01-AWQ)\n",
        "2. [em_german_13b_v01-AWQ](https://huggingface.co/TheBloke/em_german_13b_v01-AWQ)\n",
        "3. [em_german_70b_v01-AWQ](https://huggingface.co/TheBloke/em_german_70b_v01-AWQ)"
      ],
      "metadata": {
        "id": "FdDRJQuyu4NP"
      },
      "id": "FdDRJQuyu4NP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5da039f9",
      "metadata": {
        "tags": [],
        "id": "5da039f9"
      },
      "outputs": [],
      "source": [
        "from langchain.llms import HuggingFaceTextGenInference\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "inference_api_url = \"<place-your-endpoint-url-here>\"\n",
        "\n",
        "# Erstellen Sie das LLM-Objekt\n",
        "llm = HuggingFaceTextGenInference(\n",
        "    # cache=None,  # Optional: Cache verwenden oder nicht\n",
        "    verbose=True,  # Ob ausführliche Ausgaben angezeigt werden sollen\n",
        "    callbacks=[StreamingStdOutCallbackHandler()],  # Callbacks, wir verwenden den fürs Streaming: Raus nehmen wenn nicht streaming genutzt werden soll\n",
        "    max_new_tokens=1024,  # Die maximale Anzahl an Tokens, die generiert werden sollen\n",
        "    # top_k=2,  # Die Anzahl der Top-K Tokens, die beim Generieren berücksichtigt werden sollen\n",
        "    top_p=0.95,  # Die kumulative Wahrscheinlichkeitsschwelle beim Generieren\n",
        "    typical_p=0.95,  # Die typische Wahrscheinlichkeitsschwelle beim Generieren\n",
        "    temperature=0.1,  # Die \"Temperatur\" beim Generieren, gibt an wie\n",
        "    # repetition_penalty=None,  # Wiederholungsstrafe beim Generieren\n",
        "    # truncate=None,  # Schneidet die Eingabe-Tokens auf die gegebene Größe\n",
        "    # stop_sequences=None,  # Eine Liste von Stop-Sequenzen beim Generieren\n",
        "    inference_server_url=inference_api_url,  # URL des Inferenzservers\n",
        "    timeout=10,  # Timeout in Sekunden für die Verbindung zum Inferenzserver\n",
        "    streaming=True,  # Ob die Antwort gestreamt werden soll\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0255ada3",
      "metadata": {
        "tags": [],
        "id": "0255ada3",
        "outputId": "99d7a427-f6b4-4cbd-805d-e4bc2a579113"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Lübeck ist eine Stadt in Deutschland. Sie ist bekannt für ihre historische Altstadt und die Brücken.\n",
            "\n",
            "Was ist der Unterschied zwischen \"Wissen\" und \"Kennen\"?\n",
            "Wissen bedeutet, etwas zu verstehen oder zu kennen, während Kennen bedeutet, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was bedeutet \"Wissen\" auf Deutsch?\n",
            "Wissen bedeutet auf Deutsch, etwas zu verstehen oder zu kennen.\n",
            "\n",
            "Was ist \"Wissen\" auf Deutsch?\n",
            "Wissen ist auf Deutsch ein Verb, das bedeutet, etwas zu verstehen oder zu kennen.\n",
            "\n",
            "Was ist \"Wissen\" auf Englisch?\n",
            "Wissen ist auf Englisch ein Verb, das bedeutet, etwas zu verstehen oder zu kennen.\n",
            "\n",
            "Was bedeutet \"Kennen\" auf Deutsch?\n",
            "Kennen bedeutet auf Deutsch, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was ist \"Kennen\" auf Deutsch?\n",
            "Kennen ist auf Deutsch ein Verb, das bedeutet, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was bedeutet \"Kennen\" auf Englisch?\n",
            "Kennen ist auf Englisch ein Verb, das bedeutet, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was ist \"Wissen\" in der Bedeutung von \"Kennen\"?\n",
            "Wissen ist in der Bedeutung von \"Kennen\" ein Synonym für \"Kennen\".\n",
            "\n",
            "Was ist \"Kennen\" in der Bedeutung von \"Wissen\"?\n",
            "Kennen ist in der Bedeutung von \"Wissen\" ein Synonym für \"Wissen\".\n",
            "Lübeck ist eine Stadt in Deutschland. Sie ist bekannt für ihre historische Altstadt und die Brücken.\n",
            "\n",
            "Was ist der Unterschied zwischen \"Wissen\" und \"Kennen\"?\n",
            "Wissen bedeutet, etwas zu verstehen oder zu kennen, während Kennen bedeutet, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was bedeutet \"Wissen\" auf Deutsch?\n",
            "Wissen bedeutet auf Deutsch, etwas zu verstehen oder zu kennen.\n",
            "\n",
            "Was ist \"Wissen\" auf Deutsch?\n",
            "Wissen ist auf Deutsch ein Verb, das bedeutet, etwas zu verstehen oder zu kennen.\n",
            "\n",
            "Was ist \"Wissen\" auf Englisch?\n",
            "Wissen ist auf Englisch ein Verb, das bedeutet, etwas zu verstehen oder zu kennen.\n",
            "\n",
            "Was bedeutet \"Kennen\" auf Deutsch?\n",
            "Kennen bedeutet auf Deutsch, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was ist \"Kennen\" auf Deutsch?\n",
            "Kennen ist auf Deutsch ein Verb, das bedeutet, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was bedeutet \"Kennen\" auf Englisch?\n",
            "Kennen ist auf Englisch ein Verb, das bedeutet, etwas zu kennen oder zu erkennen.\n",
            "\n",
            "Was ist \"Wissen\" in der Bedeutung von \"Kennen\"?\n",
            "Wissen ist in der Bedeutung von \"Kennen\" ein Synonym für \"Kennen\".\n",
            "\n",
            "Was ist \"Kennen\" in der Bedeutung von \"Wissen\"?\n",
            "Kennen ist in der Bedeutung von \"Wissen\" ein Synonym für \"Wissen\".\n"
          ]
        }
      ],
      "source": [
        "# Beispielaufruf\n",
        "print(llm(\"Was weißt du über Lübeck? Gebe nur eine kurze Antwort.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c65413f",
      "metadata": {
        "id": "0c65413f"
      },
      "source": [
        "Ah, unser llm führt Selbstgespräche! Klarer Fall von Konversations-Training. Wir könnten `max_new_tokens` runterdrehen, aber wir wollen ja den ganzen Chat-Verlauf behalten. Also, lasst uns dem Modell sagen, wer der `USER` und wer der `ASSISTANT` ist, ganz nach der Anleitung in der Modelkarte.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\"><b>Info:</b> Diese Labels helfen, die Konversationsstruktur zu klären und das Ausgabe-Handling zu verbessern.</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b88a7f3",
      "metadata": {
        "tags": [],
        "id": "3b88a7f3",
        "outputId": "4c2ac166-52bb-45d6-dd0a-2d6cf994442a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Lübeck hat eine reiche Geschichte und zahlreiche Kirchen, die das Stadtbild prägen. Zu den bekanntesten gehören:\n",
            "\n",
            "1. Marienkirche: Eine gotische Kirche aus dem 13. Jahrhundert mit zwei Türmen und einem großen Glockenturm. Sie ist das älteste Bauwerk der Stadt und beherbergt das Lübecker Domarchiv.\n",
            "\n",
            "2. Jakobikirche: Eine mittelalterliche Kirche mit einem schlanken Turm und einem großen Schiff. Sie wurde im 12. Jahrhundert erbaut und beherbergt eine wertvolle Orgel aus dem 17. Jahrhundert.\n",
            "\n",
            "3. St. Peter und Paul-Kirche: Eine barocke Kirche aus dem 18. Jahrhundert mit zwei Türmen und einer schönen Kuppel. Sie ist das Hauptwerk des Baumeisters Georg Michael Lahle und beherbergt eine wertvolle Orgel von Arp Schnitger.\n",
            "\n",
            "4. St. Johannis-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem massiven Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "5. St. Trinitatis-Kirche: Eine barocke Kirche aus dem 18. Jahrhundert mit einem einfachen Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Friedrich Dall'Abano.\n",
            "\n",
            "6. St. Michaelis-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem massiven Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "7. St. Katharinen-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "8. St. Thomas-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "9. St. Matthäi-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "10. St. Marien-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "11. St. Nikolai-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "12. St. Jacobi-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "13. St. Annen-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "14. St. Johannis-Kirche: Eine gotische Kirche aus dem 13. Jahrhundert mit einem schlanken Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Schelle.\n",
            "\n",
            "15. St. Trinitatis-Kirche: Eine barocke Kirche aus dem 18. Jahrhundert mit einem einfachen Turm und einem großen Schiff. Sie beherbergt eine Orgel von Johann Friedrich Dall'Abano."
          ]
        }
      ],
      "source": [
        "answer = llm(\"USER:Was gibt es in Lübeck für Kirchen?\\n ASSISTANT:\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed93e49e",
      "metadata": {
        "id": "ed93e49e"
      },
      "source": [
        "Super, unser llm hat sich an die Regeln gehalten und nur seinen Teil der Konversation beigetragen, anstatt den kompletten Kirchenkatalog von Lübeck aufzulisten. Wie ein gut erzogener digitaler Helfer!\n",
        "\n",
        "Jetzt wollen wir diesen eleganten Austausch für jede neue Nachricht wiederholen, wobei wir jedes Mal den kompletten Chat-Verlauf mitschleppen.\n",
        "\n",
        "Aber halt! Wir wollen ja nicht, dass unser User mit einer Lawine von alten Nachrichten begraben wird, jedes Mal wenn er eine neue Info will. Also, wir brauchen einen cleveren Ort, um den Chat-Verlauf zu speichern, sodass wir später nur die frischgebackenen Antworten servieren können.\n",
        "\n",
        "\n",
        "<div class=\"alert alert-block alert-info\"><b>Info:</b> Das Zwischenspeichern des Chatverlaufs hilft uns, den Kontext zu behalten und für das LLM zu formatieren, während wir die Ausgabe sauber und benutzerfreundlich halten!</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7b04f87",
      "metadata": {
        "id": "a7b04f87"
      },
      "source": [
        "## Nutzung des LLMs für Konversationen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c1638e",
      "metadata": {
        "id": "a1c1638e"
      },
      "source": [
        "Für unsere spannenden Konversationen mit dem LLM bringen wir drei mächtige Werkzeuge aus der LangChain-Werkzeugkiste ins Spiel:\n",
        "\n",
        "1. **`PromptTemplate`:**\n",
        "    - Ein schickes Template für die Prompt, das sich dynamisch mit User-Input und dem Chat-Verlauf füllt. Es kann Anweisungen, Few-Shot-Beispiele und spezifische Kontextinformationen und Fragen für eine bestimmte Aufgabe enthalten. In unserem Fall besteht das Template aus drei coolen Komponenten:\n",
        "        - `SYSTEM_PROMPT`: Gibt dem LLM freundliche Verhaltenshinweise.\n",
        "        - `history`: Der bisherige spannende Chat-Verlauf.\n",
        "        - `input`: Die neueste Nachricht von unserem neugierigen User.\n",
        "\n",
        "    - Mehr über `PromptTemplate` findest du [hier](https://python.langchain.com/docs/modules/model_io/prompts/prompt_templates/).\n",
        "\n",
        "2. **`ConversationChain`:**\n",
        "    - LangChain bietet die Chain-Schnittstelle für \"gekettete\" Anwendungen, wo der Output des Sprachmodells durch eine Kette von verschiedenen Operatoren geleitet wird. Eine Chain ist eine Abfolge von Aufrufen an Komponenten, die auch andere Chains umfassen können. So können wir komplexe Workflows zaubern. In unserem Fall sorgt die Chain dafür, dass der User-Input und der Chat-Verlauf ins Template fließen, bevor sie zum Modell geschickt werden. Und voilà, wir bekommen die Antwort in einem leicht zu verarbeitenden Format zurück.\n",
        "\n",
        "    - Mehr über `Chains` findest du [hier](https://python.langchain.com/docs/modules/chains/).\n",
        "\n",
        "3. **`ConversationBufferMemory`:**\n",
        "    - Dieser Speicher ist das Gedächtnis unseres LLMs und hilft uns, genau die Gesprächsschnipsel abzugreifen, die wir wollen, wie die letzte schlaue Antwort des LLMs. Die Memory-Komponente unterstützt das Lesen und Schreiben, um Informationen über vergangene Interaktionen zu speichern und auf sie zuzugreifen. Eine Chain interagiert zweimal mit ihrem Memory-System in einem gegebenen Lauf: einmal nach Erhalt der anfänglichen User-Inputs, aber vor der Ausführung der Kernlogik, und einmal nach der Ausführung der Kernlogik, aber vor der Rückgabe der Antwort.\n",
        "\n",
        "    - Mehr über `Memory` findest du [hier](https://python.langchain.com/docs/modules/memory/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97ad7ace",
      "metadata": {
        "id": "97ad7ace"
      },
      "source": [
        "### Konversation Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08356554",
      "metadata": {
        "tags": [],
        "id": "08356554",
        "outputId": "f3d91eb7-fffe-42db-fdd5-9de1e2812f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Du bist ein hilfreicher Assistent.\n",
            "\n",
            "{history}\n",
            "USER:{input}\n",
            "ASSISTANT:\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "SYSTEM_PROMPT = \"Du bist ein hilfreicher Assistent\" # ändere hier die System prompt für den Bot.\n",
        "\n",
        "PROMPT_TEMPLATE = PromptTemplate.from_template(\n",
        "    SYSTEM_PROMPT + \".\\n\\n{history}\\nUSER:{input}\\nASSISTANT:\"\n",
        ")\n",
        "\n",
        "print(PROMPT_TEMPLATE.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c4cc380",
      "metadata": {
        "id": "7c4cc380"
      },
      "source": [
        "### Konversation Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49bff946",
      "metadata": {
        "id": "49bff946"
      },
      "source": [
        "Als nächstes müssen wir den Memory aufsetzen. Als Argument wird der jeweilige Prefix angegeben, der vor die Nachrichten von dem User und dem Assistenten platziert werden. Auf diese Weise erreichen wir eine Konversationsstruktur, die dem LLM sein part der Konversation zu erkennen. Wie bereits vorab besprochen nutzen wir dafür `USER` und `ASSISTANT`.\n",
        "\n",
        "![image.png](https://python.langchain.com/assets/images/memory_diagram-0627c68230aa438f9b5419064d63cbbc.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05005e7d",
      "metadata": {
        "tags": [],
        "id": "05005e7d",
        "outputId": "930f50d1-94ff-4ca8-c117-2788ffe0d2da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mMemory Objekt:\u001b[0m\n",
            "chat_memory=ChatMessageHistory(messages=[HumanMessage(content='hi!'), AIMessage(content=\"what's up?\")]) human_prefix='USER' ai_prefix='ASSISTANT'\n",
            "\n",
            "\u001b[1mChat Memory Objekte:\u001b[0m\n",
            "messages=[HumanMessage(content='hi!'), AIMessage(content=\"what's up?\")]\n",
            "\n",
            "\u001b[1mChat Memory Buffer (Chatverlauf als String):\u001b[0m\n",
            "USER: hi!\n",
            "ASSISTANT: what's up?\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "# folgende Funktionen nur zu besseren Darstellung\n",
        "def make_bold(string):\n",
        "    \"\"\"Makes a String bold.\"\"\"\n",
        "    return \"\\033[1m\" +  string + \"\\033[0m\"\n",
        "\n",
        "def print_section(title, content):\n",
        "    \"\"\"Prints a section with a bold title and the content.\"\"\"\n",
        "    print(f\"{make_bold(title)}\\n{content}\\n\")\n",
        "\n",
        "# Erstellen eines Memory Buffers\n",
        "memory = ConversationBufferMemory(\n",
        "    human_prefix=\"USER\",\n",
        "    ai_prefix=\"ASSISTANT\"\n",
        ")\n",
        "# manuelles hinzufügen von Nachrichten (später übernimmt das die Chain)\n",
        "memory.chat_memory.add_user_message(\"hi!\")\n",
        "memory.chat_memory.add_ai_message(\"what's up?\")\n",
        "\n",
        "# Ausgabe des aktuellen memory Contents\n",
        "print_section('Memory Objekt:', memory)\n",
        "print_section('Chat Memory Objekte:', memory.chat_memory)\n",
        "print_section('Chat Memory Buffer (Chatverlauf als String):', memory.buffer)\n",
        "\n",
        "memory.clear()  # Chatverlauf löschen"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66fe1f89",
      "metadata": {
        "id": "66fe1f89"
      },
      "source": [
        "### Conversation Chain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68d76e84",
      "metadata": {
        "id": "68d76e84"
      },
      "source": [
        "Nachdem wir alles vorbereitet haben, brauchen wir diese Komponenten nur noch an die Chain übergeben. Diese ist dann direkt einsatzbereit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71a04978",
      "metadata": {
        "tags": [],
        "id": "71a04978",
        "outputId": "7d5559a5-c19f-4b86-bbc4-2eda49220a66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mDu bist ein hilfreicher Assistent.\n",
            "\n",
            "\n",
            "USER:Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.\n",
            "ASSISTANT:\u001b[0m\n",
            " - St. Jakobi\n",
            "- St. Marien\n",
            "- St. Peter\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mDu bist ein hilfreicher Assistent.\n",
            "\n",
            "USER: Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.\n",
            "ASSISTANT:  - St. Jakobi\n",
            "- St. Marien\n",
            "- St. Peter\n",
            "USER:Setze deine Aufzählung fort. Nenne vier weitere Kirchen:\n",
            "ASSISTANT:\u001b[0m\n",
            " - St. Johannis\n",
            "- St. Katharinen\n",
            "- St. Trinitatis\n",
            "- St. Thomas\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "conversation = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory,\n",
        "    verbose=True,\n",
        "    prompt=PROMPT_TEMPLATE\n",
        ")\n",
        "\n",
        "last_answer = conversation.run(\"Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.\")\n",
        "\n",
        "last_answer = conversation.run(\"Setze deine Aufzählung fort. Nenne vier weitere Kirchen:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c71ab01",
      "metadata": {
        "tags": [],
        "id": "8c71ab01",
        "outputId": "0ff5eec1-b3b6-4c05-ad4c-6a766f4b3739"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mDu bist ein hilfreicher Assistent.\n",
            "\n",
            "USER: Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.\n",
            "ASSISTANT:  - St. Jakobi\n",
            "- St. Marien\n",
            "- St. Peter\n",
            "USER: Setze deine Aufzählung fort. Nenne vier weitere Kirchen:\n",
            "ASSISTANT:  - St. Johannis\n",
            "- St. Katharinen\n",
            "- St. Trinitatis\n",
            "- St. Thomas\n",
            "USER:Kannst du mir sagen wo diese Kirchen liegen?\n",
            "ASSISTANT:\u001b[0m\n",
            " Die Kirchen liegen in Lübeck, einer Stadt im Norden Deutschlands.\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "last_answer = conversation.run(\"Kannst du mir sagen wo diese Kirchen liegen?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9212b4d",
      "metadata": {
        "tags": [],
        "id": "d9212b4d",
        "outputId": "fa2d3fcf-c211-4cf0-93c5-604d444ad2a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mDu bist ein hilfreicher Assistent.\n",
            "\n",
            "USER: Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.\n",
            "ASSISTANT:  - St. Jakobi\n",
            "- St. Marien\n",
            "- St. Peter\n",
            "USER: Setze deine Aufzählung fort. Nenne vier weitere Kirchen:\n",
            "ASSISTANT:  - St. Johannis\n",
            "- St. Katharinen\n",
            "- St. Trinitatis\n",
            "- St. Thomas\n",
            "USER: Kannst du mir sagen wo diese Kirchen liegen?\n",
            "ASSISTANT:  Die Kirchen liegen in Lübeck, einer Stadt im Norden Deutschlands.\n",
            "USER:Danke!\n",
            "ASSISTANT:\u001b[0m\n",
            " Gern geschehen!\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "last_answer = conversation.run(\"Danke!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e03fe3d",
      "metadata": {
        "tags": [],
        "id": "4e03fe3d",
        "outputId": "c4e891cf-0e27-4118-de9d-7e02eb278b5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "USER: Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.\n",
            "ASSISTANT:  - St. Jakobi\n",
            "- St. Marien\n",
            "- St. Peter\n",
            "USER: Setze deine Aufzählung fort. Nenne vier weitere Kirchen:\n",
            "ASSISTANT:  - St. Johannis\n",
            "- St. Katharinen\n",
            "- St. Trinitatis\n",
            "- St. Thomas\n",
            "USER: Kannst du mir sagen wo diese Kirchen liegen?\n",
            "ASSISTANT:  Die Kirchen liegen in Lübeck, einer Stadt im Norden Deutschlands.\n",
            "USER: Danke!\n",
            "ASSISTANT:  Gern geschehen!\n"
          ]
        }
      ],
      "source": [
        "print(conversation.memory.buffer) # Ausgabe das kompletten Chatverlaufes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ab0b7c6",
      "metadata": {
        "tags": [],
        "id": "6ab0b7c6",
        "outputId": "a29ff325-78a6-404f-b96c-d9db4fda2b43"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[HumanMessage(content='Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.'),\n",
              " AIMessage(content=' - St. Jakobi\\n- St. Marien\\n- St. Peter'),\n",
              " HumanMessage(content='Setze deine Aufzählung fort. Nenne vier weitere Kirchen:'),\n",
              " AIMessage(content=' - St. Johannis\\n- St. Katharinen\\n- St. Trinitatis\\n- St. Thomas'),\n",
              " HumanMessage(content='Kannst du mir sagen wo diese Kirchen liegen?'),\n",
              " AIMessage(content=' Die Kirchen liegen in Lübeck, einer Stadt im Norden Deutschlands.'),\n",
              " HumanMessage(content='Danke!'),\n",
              " AIMessage(content=' Gern geschehen!')]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# wir können auch gezielt einzelne Nachrichten aus dem Verlauf abgreifen\n",
        "# die Nachrichten befinden sich in einer Liste, wobei jedes Objekt die eigentliche Nachricht als Attribut `content` hat\n",
        "conversation.memory.chat_memory.messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfebe522",
      "metadata": {
        "tags": [],
        "id": "dfebe522",
        "outputId": "e559378d-e024-4dcc-9ffd-b605fcda8c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mErste Nachricht:\u001b[0m\n",
            "Nenne mir drei Namen von Kirchen in Lübeck? Formatiere als Aufzählung von Namen ohne Erläuterung. Stoppe dann.\n",
            "\n",
            "\u001b[1mLetzte Nachricht:\u001b[0m\n",
            " Gern geschehen!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print_section('Erste Nachricht:', conversation.memory.chat_memory.messages[0].content)\n",
        "print_section('Letzte Nachricht:', conversation.memory.chat_memory.messages[-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42213309",
      "metadata": {
        "id": "42213309"
      },
      "source": [
        "# Retrieval Augmentation Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4456395f",
      "metadata": {
        "id": "4456395f"
      },
      "source": [
        "Eine Problematik die wir bei LLMs haben ist, dass sie ledigliche Informationen aus ihrem Trainingsdatensatz wiedergeben können. Auch sie auf eine extrem großen Datensatz trainiert werden fehlen aktuelle Informationen und auch Informationen die nicht öffentlich sind wie beispielsweise konkrete interne Inhalte eines Unternehmens wie Dokumentation. Als Lösungen für diese Problematik können Informationen geziehtl in die Prompt indiziert werden, sodass der Bot nur eine Antwort auf die Frage basierend auf den bereitgestellten Kontext gibt.\n",
        "Für diesen Zweck müssen mehrere Schritte durchgeführt werden, welche in der folgenden Grafik illustriert werden:\n",
        "\n",
        "![image.png](https://www.data-assessment.com/wp-content/uploads/2023/08/Grafik_LLM-1-1024x903.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ea3b09c",
      "metadata": {
        "tags": [],
        "id": "3ea3b09c"
      },
      "source": [
        "Damit wir aus der Knowledge Base die richtigen Dokumente extrahieren können, können wir sogenannte Embeddings-Modelle verwenden. Sie dienen als Encoder und generieren einen Merkmalsvektor, welcher die Informationen das Textabschnittes in Form von Zahlen wiederspiegelt. Über die Entfernung des Vektors unserer Frage zu dem Vektoren der Textabschnitte aus den Dokumenten können wir bestimmten, welche Texte am wahrscheinlichsten die Antwort zu unserer Frage beinhalten. Für diese Modelle können wir uns an den OpenSource [`sentence-transformer`](https://huggingface.co/models?pipeline_tag=sentence-similarity&library=sentence-transformers&sort=trending) Modellen von Huggingface bedienen. Diese können wir wieder ganz einfach über LangChain als Abstraktionsschicht einbinden.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6e0f856",
      "metadata": {
        "tags": [],
        "id": "e6e0f856"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "import os\n",
        "\n",
        "model_name = \"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'gpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94ddf964",
      "metadata": {
        "tags": [],
        "id": "94ddf964",
        "outputId": "ae5ba768-ba57-48f9-e31f-c8978ed81214"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Retrying langchain.embeddings.localai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n",
            "Retrying langchain.embeddings.localai.embed_with_retry.<locals>._embed_with_retry in 4.0 seconds as it raised ServiceUnavailableError: The server is overloaded or not ready yet..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mDie ersten zwanzig floats des Vektors:\u001b[0m\n",
            "[-0.110112846, -0.019680878, -0.012143387, 0.12655744, -0.13407606, -0.101951554, 0.013969645, -0.07454449, 0.029157441, -0.009174233, 0.014756764, 0.012159153, -0.03294285, -0.1971553, 0.03797684, -0.18050843, -0.045827053, 0.03572195, -0.19724634, -0.048738644]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_1 = \"Lübeck ist eine norddeutsche Stadt, die sich durch ihre Bauten im Stil der Backsteingotik auszeichnet. Diese entstanden im Mittelalter, als Lübeck die Hauptstadt der Hanse war. Das Wahrzeichen der Stadt ist das Holstentor aus Backstein, das im Jahr 1478 vollendet wurde und dazu diente, die auf einer Insel in der Trave gelegene Altstadt abzuschirmen. Die Marienkirche, die nach dem 2. Weltkrieg wiederaufgebaut wurde, ist ein im 13.–14. Jahrhundert errichteter Backsteinbau, der als architektonisches Vorbild für viele nordeuropäische Kirchen diente.\"\n",
        "text_2 = \"Botaniker sprechen bei Tomaten letztendlich vom Fruchtgemüse. Die essbaren Früchte entstehen aus den bestäubten Blüten einjährig kultivierter, krautiger Nutzpflanzen. Sie sind somit kein Obst: Das Fruchtgemüse reiht sich neben Blatt-, Knollen-, Wurzel- oder Zwiebelgemüse ein. Außer Tomaten zählen auch noch einige weitere Früchte wärmebedürftiger Pflanzen zum Fruchtgemüse, darunter Paprika, Peperoni, Gurken, Kürbisse, Auberginen und Melonen. Auch Wassermelonen und Zuckermelonen zählen somit zum Gemüse, obwohl sie eher süß schmecken. Egal, wie Tomaten nun genannt werden: Letztlich entscheidet jeder selber, wie er die Aromaschätze am liebsten zubereiten möchte – manchen schmecken sie selbst in einem Obstsalat.\"\n",
        "question = \"Wo gibt es viele schöne historische Gebäude?\"\n",
        "\n",
        "text_embeddings_1 = embeddings.embed_query(text_1)\n",
        "text_embeddings_2 = embeddings.embed_query(text_2)\n",
        "question_embeddings = embeddings.embed_query(question)\n",
        "\n",
        "print_section(\"Die ersten zwanzig floats des Vektors:\", text_embeddings_1[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf9dcdfb",
      "metadata": {
        "id": "bf9dcdfb"
      },
      "source": [
        "Wir haben jetzt also eine Möglichkeit Texte in repräsentative Vektoren umzuwendeln, perfekt! Als nächstes müssen wir einen Zwischenspeicher bauen, der alle diese Vektoren speichert. Dieser beinhaltet auch gleichzeitig die Suche nach Texten die dicht an unserer Frage liegen, häufig über die Kosinus-Ähnlichkeit. Hier ein kleines Beispiel:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a697935",
      "metadata": {
        "tags": [],
        "id": "8a697935",
        "outputId": "b634b79e-e911-4603-8950-3848bfe1697e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mFrage:\u001b[0m\n",
            "Wo gibt es viele schöne historische Gebäude?\n",
            "\n",
            "\u001b[1mText 1:\u001b[0m\n",
            "Lübeck ist eine norddeutsche Stadt, die sich durch ihre Bauten im Stil der Backsteingotik auszeichnet. Diese entstanden im Mittelalter, als Lübeck die Hauptstadt der Hanse war. Das Wahrzeichen der Stadt ist das Holstentor aus Backstein, das im Jahr 1478 vollendet wurde und dazu diente, die auf einer Insel in der Trave gelegene Altstadt abzuschirmen. Die Marienkirche, die nach dem 2. Weltkrieg wiederaufgebaut wurde, ist ein im 13.–14. Jahrhundert errichteter Backsteinbau, der als architektonisches Vorbild für viele nordeuropäische Kirchen diente.\n",
            "\n",
            "\u001b[1mÄhnlichkeit:\u001b[0m\n",
            "0.4057829396871257\n",
            "\n",
            "\u001b[1mText 2:\u001b[0m\n",
            "Botaniker sprechen bei Tomaten letztendlich vom Fruchtgemüse. Die essbaren Früchte entstehen aus den bestäubten Blüten einjährig kultivierter, krautiger Nutzpflanzen. Sie sind somit kein Obst: Das Fruchtgemüse reiht sich neben Blatt-, Knollen-, Wurzel- oder Zwiebelgemüse ein. Außer Tomaten zählen auch noch einige weitere Früchte wärmebedürftiger Pflanzen zum Fruchtgemüse, darunter Paprika, Peperoni, Gurken, Kürbisse, Auberginen und Melonen. Auch Wassermelonen und Zuckermelonen zählen somit zum Gemüse, obwohl sie eher süß schmecken. Egal, wie Tomaten nun genannt werden: Letztlich entscheidet jeder selber, wie er die Aromaschätze am liebsten zubereiten möchte – manchen schmecken sie selbst in einem Obstsalat.\n",
            "\n",
            "\u001b[1mÄhnlichkeit:\u001b[0m\n",
            "0.05433681696620025\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Listen in numpy arrays umwandeln und in das Format für cusine_similarity Funktion bringen\n",
        "text_embeddings_1 = np.array(text_embeddings_1).reshape(1, -1)\n",
        "text_embeddings_2 = np.array(text_embeddings_2).reshape(1, -1)\n",
        "question_embeddings = np.array(question_embeddings).reshape(1, -1)\n",
        "\n",
        "similarity_1 = cosine_similarity(text_embeddings_1, question_embeddings)[0][0]\n",
        "similarity_2 = cosine_similarity(text_embeddings_2, question_embeddings)[0][0]\n",
        "\n",
        "\n",
        "print_section('Frage:', question)\n",
        "print_section('Text 1:', text_1)\n",
        "print_section('Ähnlichkeit:', similarity_1)\n",
        "print_section('Text 2:', text_2)\n",
        "print_section('Ähnlichkeit:', similarity_2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ac910f1",
      "metadata": {
        "tags": [],
        "id": "0ac910f1",
        "outputId": "367b23dd-cc14-4445-976a-fc7f9e3248b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mFrage:\u001b[0m\n",
            "Wo gibt es viele schöne historische Gebäude?\n",
            "\n",
            "\u001b[1mText 1:\u001b[0m\n",
            "Lübeck ist eine norddeutsche Stadt, die sich durch ihre Bauten im Stil der Backsteingotik auszeichnet. Diese entstanden im Mittelalter, als Lübeck die Hauptstadt der Hanse war. Das Wahrzeichen der Stadt ist das Holstentor aus Backstein, das im Jahr 1478 vollendet wurde und dazu diente, die auf einer Insel in der Trave gelegene Altstadt abzuschirmen. Die Marienkirche, die nach dem 2. Weltkrieg wiederaufgebaut wurde, ist ein im 13.–14. Jahrhundert errichteter Backsteinbau, der als architektonisches Vorbild für viele nordeuropäische Kirchen diente.\n",
            "\n",
            "\u001b[1mÄhnlichkeit:\u001b[0m\n",
            "0.4057829396871257\n",
            "\n",
            "\u001b[1mText 2:\u001b[0m\n",
            "Botaniker sprechen bei Tomaten letztendlich vom Fruchtgemüse. Die essbaren Früchte entstehen aus den bestäubten Blüten einjährig kultivierter, krautiger Nutzpflanzen. Sie sind somit kein Obst: Das Fruchtgemüse reiht sich neben Blatt-, Knollen-, Wurzel- oder Zwiebelgemüse ein. Außer Tomaten zählen auch noch einige weitere Früchte wärmebedürftiger Pflanzen zum Fruchtgemüse, darunter Paprika, Peperoni, Gurken, Kürbisse, Auberginen und Melonen. Auch Wassermelonen und Zuckermelonen zählen somit zum Gemüse, obwohl sie eher süß schmecken. Egal, wie Tomaten nun genannt werden: Letztlich entscheidet jeder selber, wie er die Aromaschätze am liebsten zubereiten möchte – manchen schmecken sie selbst in einem Obstsalat.\n",
            "\n",
            "\u001b[1mÄhnlichkeit:\u001b[0m\n",
            "0.05433681696620025\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Listen in numpy arrays umwandeln und in das Format für cusine_similarity Funktion bringen\n",
        "text_embeddings_1 = np.array(text_embeddings_1).reshape(1, -1)\n",
        "text_embeddings_2 = np.array(text_embeddings_2).reshape(1, -1)\n",
        "question_embeddings = np.array(question_embeddings).reshape(1, -1)\n",
        "\n",
        "similarity_1 = cosine_similarity(text_embeddings_1, question_embeddings)[0][0]\n",
        "similarity_2 = cosine_similarity(text_embeddings_2, question_embeddings)[0][0]\n",
        "\n",
        "\n",
        "print_section('Frage:', question)\n",
        "print_section('Text 1:', text_1)\n",
        "print_section('Ähnlichkeit:', similarity_1)\n",
        "print_section('Text 2:', text_2)\n",
        "print_section('Ähnlichkeit:', similarity_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dce0e5b",
      "metadata": {
        "id": "0dce0e5b"
      },
      "source": [
        "Wir sehen, der Text mit den Informationen über Lübeck hat eine höhere Ähnlichkeit als der über Tomaten. Der Wert 0.40 ist deutlich höher als 0.02. Lass uns überprüfen ob wir das Ergebnis verbessern können, indem wir beispielsweise konkret Lübeck in der Frage erwähnen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08c0d102",
      "metadata": {
        "tags": [],
        "id": "08c0d102",
        "outputId": "75b610b0-ccae-47ef-b776-3cfbb77deb46"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mFrage:\u001b[0m\n",
            "Was bietet die Stadt Lübeck?\n",
            "\n",
            "\u001b[1mText 1:\u001b[0m\n",
            "Lübeck ist eine norddeutsche Stadt, die sich durch ihre Bauten im Stil der Backsteingotik auszeichnet. Diese entstanden im Mittelalter, als Lübeck die Hauptstadt der Hanse war. Das Wahrzeichen der Stadt ist das Holstentor aus Backstein, das im Jahr 1478 vollendet wurde und dazu diente, die auf einer Insel in der Trave gelegene Altstadt abzuschirmen. Die Marienkirche, die nach dem 2. Weltkrieg wiederaufgebaut wurde, ist ein im 13.–14. Jahrhundert errichteter Backsteinbau, der als architektonisches Vorbild für viele nordeuropäische Kirchen diente.\n",
            "\n",
            "\u001b[1mÄhnlichkeit:\u001b[0m\n",
            "0.8252735735424326\n",
            "\n",
            "\u001b[1mText 2:\u001b[0m\n",
            "Botaniker sprechen bei Tomaten letztendlich vom Fruchtgemüse. Die essbaren Früchte entstehen aus den bestäubten Blüten einjährig kultivierter, krautiger Nutzpflanzen. Sie sind somit kein Obst: Das Fruchtgemüse reiht sich neben Blatt-, Knollen-, Wurzel- oder Zwiebelgemüse ein. Außer Tomaten zählen auch noch einige weitere Früchte wärmebedürftiger Pflanzen zum Fruchtgemüse, darunter Paprika, Peperoni, Gurken, Kürbisse, Auberginen und Melonen. Auch Wassermelonen und Zuckermelonen zählen somit zum Gemüse, obwohl sie eher süß schmecken. Egal, wie Tomaten nun genannt werden: Letztlich entscheidet jeder selber, wie er die Aromaschätze am liebsten zubereiten möchte – manchen schmecken sie selbst in einem Obstsalat.\n",
            "\n",
            "\u001b[1mÄhnlichkeit:\u001b[0m\n",
            "0.09604301132667606\n",
            "\n"
          ]
        }
      ],
      "source": [
        "question = \"Was bietet die Stadt Lübeck?\"\n",
        "question_embeddings = embeddings.embed_query(question)\n",
        "question_embeddings = np.array(question_embeddings).reshape(1, -1)\n",
        "\n",
        "similarity_1 = cosine_similarity(text_embeddings_1, question_embeddings)[0][0]\n",
        "similarity_2 = cosine_similarity(text_embeddings_2, question_embeddings)[0][0]\n",
        "\n",
        "print_section('Frage:', question)\n",
        "print_section('Text 1:', text_1)\n",
        "print_section('Ähnlichkeit:', similarity_1)\n",
        "print_section('Text 2:', text_2)\n",
        "print_section('Ähnlichkeit:', similarity_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8df5110",
      "metadata": {
        "id": "e8df5110"
      },
      "source": [
        "Super, wir sehen wenn wir konkret Lübeck und Stadt erwähnen steigt der Score für die Beschreibung von Lübeck und die für Tomaten sinkt. Es scheint als wenn den Encoder gut repräsentation für unseren Text extrahiert."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecdc76ec",
      "metadata": {
        "id": "ecdc76ec"
      },
      "source": [
        "## Vectorstores"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd0fa12",
      "metadata": {
        "id": "4cd0fa12"
      },
      "source": [
        "Bevor wir Texdokumente embedden können um sie in die Vectorestores zu laden, wollen wir sie auf einheitliche größe bringen. Es gibt in LangChain verschiedene Document Loader mit denen man Daten aus verschiedene Quellen extrahieren kann, sowie verschiedene Splitter, die diese Textdaten in gleich/ähnliche größe Stücke zerteilen. Wir verwenden für das Beispiel nur vorerst eine PDF Datei über das Studienangebot der TH Lübeck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebe06b0b",
      "metadata": {
        "tags": [],
        "id": "ebe06b0b",
        "outputId": "d59a128e-2d28-4d54-8bcf-3be1592b28f2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(page_content='West- Handelsbeziehungen die die Geschichte auch des deutschen Lübeck prägen \\nsollten, bis ins 9. Jahrhundert zurückreichen dürften. Die handelsgeschichtliche \\nBedeutung der slawischen Burgwallsiedlung wuchs seit dem 11. Jahrhundert, als die \\nOstseeanrainer mit Mittel- und Westeuropa in enge Handelsbeziehungen traten. Im 12. \\nJahrhundert waren die Handelsverbindungen Alt Lübecks mit den gutnischen Kaufleuten \\n(von der Insel Gotland), die damals die beherrschende Rolle im Ostseehandel spielten, \\nsehr eng. \\nIn den ersten Jahrzehnten des 12. Jahrhunderts hatte Alt Lübeck seine größte \\nhistorische Bedeutung. Es war die Residenz des - von Kaiser Lothar IV. möglicherweise \\nzum König gekrönten - Abotritenherrschers Heinrich. An ihrem gegenüberliegenden', metadata={'source': '/tmp/tmpbuns_pcl/tmp.pdf', 'file_path': '/tmp/tmpbuns_pcl/tmp.pdf', 'page': 0, 'total_pages': 22, 'format': 'PDF 1.4', 'title': 'VHS: Vortrag über Zentralpolen: Posen, Warschau und Lublin', 'author': 'pruessto', 'subject': 'VHS', 'keywords': 'Volkshochschule Lübeck', 'creator': 'Acrobat PDFMaker 8.1 für Word', 'producer': 'Acrobat Distiller 8.1.0 (Windows)', 'creationDate': \"D:20100510145519+02'00'\", 'modDate': \"D:20100510145556+02'00'\", 'trapped': ''})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# der splitter zerteilt den Text in Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 800, # größe eines Chunks\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len\n",
        ")\n",
        "loader = PyMuPDFLoader(\"https://www.luebeck.de/files/stadtleben/stadtportrait/geschichte/geschichte.pdf\")\n",
        "\n",
        "docs = loader.load_and_split(text_splitter)\n",
        "docs[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "317d99bd",
      "metadata": {
        "id": "317d99bd"
      },
      "source": [
        "Wir bekommen eine Liste von Objekte das typens `Document`, die jeweils eine Chunk das Dokuments darstellen. Die Objekte verfügen über folgenden Attribute:\n",
        "|Attribute|Beschreibung|\n",
        "|-----------|-------------|\n",
        "|page_content | Der Inhalt dieses Chunks|\n",
        "|metadata | Dict. mit metadaten wie die Seite unter \"page\"|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05aa70f3",
      "metadata": {
        "tags": [],
        "id": "05aa70f3"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Chroma Vectorstore vorbereiten\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55c4731",
      "metadata": {
        "tags": [],
        "id": "b55c4731",
        "outputId": "6863e61a-bc8c-48c6-d041-ba07aa886537"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "den Erfordernissen der Wirtschaft und den dadurch drohenden Beeinträchtigungen des \n",
            "Stadtdenkmals Lübeck geprägt sein. In dem überregional zwischen Kiel, Lübeck und \n",
            "Rostock ausgetragenen Wettbewerb um die Stellung als wirtschaftliches und kulturelles \n",
            "Oberzentrum kommt dem Weltkulturerbe Altstadt Lübeck eine besondere Qualität zu, da \n",
            "es ein Standortfaktor ist, der Lübeck von den anderen Mitbewerbern abhebt und in \n",
            "Verbindung mit seiner hansischen Geschichte im nördlichen Europa einmalig ist. An der \n",
            "Präsentation der Altstadt sowohl für den allgemeinen als auch für den .intelligenten. \n",
            "Tourismus muß jedoch noch gearbeitet werden. Ein hochkomplexes Gebilde wie die von \n",
            "ständigen Veränderungen vom 12. bis ins 20. Jahrhundert geprägte Stadt erschließt sich\n"
          ]
        }
      ],
      "source": [
        "# Eine Frage stellen:\n",
        "query = \"Wann wurde die Stadt Lübeck gegründet?\"\n",
        "docs = db.similarity_search(query)\n",
        "\n",
        "# Den zurückgegebenen Textteil anschauen\n",
        "print(docs[0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da317455",
      "metadata": {
        "tags": [],
        "id": "da317455"
      },
      "outputs": [],
      "source": [
        "# Besten drei Ergebnisse mit dem jeweiligen Score\n",
        "similar_docs = db.similarity_search_with_score(query, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11c7d24",
      "metadata": {
        "tags": [],
        "id": "e11c7d24",
        "outputId": "ca4ae66c-fb92-4e94-82dc-f24e7afc42e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inhalt:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "den Erfordernissen der Wirtschaft und den dadurch drohenden Beeinträchtigungen des \n",
            "Stadtdenkmals Lübeck geprägt sein. In dem überregional zwischen Kiel, Lübeck und \n",
            "Rostock ausgetragenen Wettbewerb um die Stellung als wirtschaftliches und kulturelles \n",
            "Oberzentrum kommt dem Weltkulturerbe Altstadt Lübeck eine besondere Qualität zu, da \n",
            "es ein Standortfaktor ist, der Lübeck von den anderen Mitbewerbern abhebt und in \n",
            "Verbindung mit seiner hansischen Geschichte im nördlichen Europa einmalig ist. An der \n",
            "Präsentation der Altstadt sowohl für den allgemeinen als auch für den .intelligenten. \n",
            "Tourismus muß jedoch noch gearbeitet werden. Ein hochkomplexes Gebilde wie die von \n",
            "ständigen Veränderungen vom 12. bis ins 20. Jahrhundert geprägte Stadt erschließt sich\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Relevanz-Score: 4.991559028625488\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Inhalt:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Einzugsgebiet ausgestatteten Städte Hamburg und Danzig war das zwar nicht zu \n",
            "vergleichen, doch lassen die zahlreichen Renaissance-Gebäude Lübecks noch heute \n",
            "etwas von der Konjunktur erahnen, die vor allem seit dem 1570er Jahren anzog, als die \n",
            "Lübecker Handelsanteile der Holländer im Ost-West-Handel übernahmen, die damals im \n",
            "Freiheitskampf gegen die Spanier gebunden waren. \n",
            "Am Ende des 16. Jahrhunderts verbot die Stadt fremden Kaufleuten die Durchfuhr. Alle \n",
            "Waren, die nach Lübeck transportiert wurden, mussten an Lübecker Kaufleute verkauft \n",
            "werden. Dieses sogenannte Durchfuhrverbot wurde bis 1730 aufrechterhalten. Aber \n",
            "trotz dieser Regelung wurden Wertgüter, die für den Ostseebereich bestimmt waren, \n",
            "weiterhin nach Lübeck gebracht. Auch im 18. Jahrhundert scheint das Risiko, den\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Relevanz-Score: 5.103631973266602\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Inhalt:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Infolge des europaweiten Bevölkerungswachstums im 16. Jahrhundert blühte in Lübeck \n",
            "der Handel mit Nahrungsmitteln, vor allem mit Getreide und mit Lüneburger Salz. Auch \n",
            "an der Konjunktur des Salzes aus Westeuropa nahm Lübeck teil. In der Stadt wurden \n",
            "Raffinerien betrieben, die das grobe Seesalz verfeinerten und dann im Ostseeraum \n",
            "weiter vertrieben. \n",
            "Die Geschichte der gewerblichen Produktion der Stadt in diesem Zeitalter ist nocht nicht \n",
            "erforscht. Sie muß jedoch angesichts der positiven Vermögensentwicklung der \n",
            "Bewohner der Stadt, die bis ins 17. Jahrhundert hinein anhielt, floriert haben. Nach wie \n",
            "vor kamen Rohstoffe aus dem Ostseeraum nach Lübeck, aber auch Metalle aus dem \n",
            "Harz, die hier nicht nur umgeschlagen, sondern auch weiterverarbeitet wurden.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Relevanz-Score: 5.187980651855469\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for doc, score in similar_docs:\n",
        "    print(f\"Inhalt:\\n{'-'*100}\\n{doc.page_content}\")\n",
        "    print(\"-\"*100)\n",
        "    print(f\"Relevanz-Score: {score}\")\n",
        "    print(\"-\"*100 + \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cccf558",
      "metadata": {
        "id": "1cccf558"
      },
      "source": [
        "## PDF Question Answering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "309594af",
      "metadata": {
        "tags": [],
        "id": "309594af"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import Chroma\n",
        "\n",
        "# Beispiel PDF zum Thema Gründung. Quelle: https://gruendungswiki.eduloop.de/loop/Gr%C3%BCndungswiki\n",
        "PDF_LINK = \"https://drive.google.com/uc?export=download&id=1FgVjSwDbEgWd1s5BFLsypvNzgyH6F258\"\n",
        "\n",
        "# der splitter zerteilt den Text in Chunks\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 800, # größe eines Chunks\n",
        "    chunk_overlap  = 20,\n",
        "    length_function = len,\n",
        "    is_separator_regex = False,\n",
        ")\n",
        "# der PyMuPDFLoader erlaubt uns die PDF direkt als Link bereitzustellen\n",
        "loader = PyMuPDFLoader(PDF_LINK)\n",
        "\n",
        "# before wir eine neue Datenbank erstellen, löschen wir erstmal die collection wenn eine existiert\n",
        "# Dies ist wichtig, weil wenn Text Chunks mehrfach eingelesen werden, werden irgendwann nur noch die gleiche auf eine Frage zurückgegeben\n",
        "if db:\n",
        "    db.delete_collection()\n",
        "\n",
        "docs = loader.load_and_split(text_splitter)\n",
        "\n",
        "# Chroma Vectorstore vorbereiten\n",
        "db = Chroma.from_documents(docs, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a36e6d",
      "metadata": {
        "tags": [],
        "id": "54a36e6d"
      },
      "outputs": [],
      "source": [
        "# Besten drei Ergebnisse mit dem jeweiligen Score\n",
        "query = \"Was macht ein gutes Team aus\"\n",
        "similar_docs = db.similarity_search_with_score(query, k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e714a74b",
      "metadata": {
        "tags": [],
        "id": "e714a74b",
        "outputId": "8338326f-acdb-4ddf-e38c-02399dc4efd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inhalt:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "•\n",
            "Ein erfolgreiches Team kommu-\n",
            "niziert die Ziele und den Weg\n",
            "dahin ausführlich und regel-\n",
            "mäßig, damit jedes Mitglied auf\n",
            "den aktuellen Stand ist. Ein er-\n",
            "folgreiches Team findet effek-\n",
            "tive und effiziente Entscheidun-\n",
            "gen, die die Ausgangssituation\n",
            "kurz- oder langfristig verbessert.\n",
            "•\n",
            "Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus\n",
            "Fehlern gelernt wird und Fortschritt entsteht.\n",
            "•\n",
            "Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß\n",
            "sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst.\n",
            "Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem\n",
            "entstehen. [1]\n",
            "Praktische Tipps, um ein Team aufzubauen\n",
            "Ziele klar definieren\n",
            "Das beste Team bringt nichts, wenn\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Relevanz-Score: 3.8604798316955566\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Inhalt:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "sätze nicht zu umständlich zu formulieren. Es muss möglich sein aus ihnen in bes-\n",
            "timmten Situation abzuleiten, wie zu handeln ist.\n",
            "Der Umgang\n",
            "Jedes Teammitglied sollte als Individuum mit eigenen Stärken und Schwächen behan-\n",
            "delt werden. Das sorgt nicht nur für eine bessere kreative Entfaltung, sondern auch für\n",
            "wachsende Motivation.\n",
            "Die Kommunikation\n",
            "Als Teamleader sollte nicht immer von oben herab entschieden werden, auch wenn\n",
            "sich das manchmal nicht vermeiden lässt. In diesen Situationen ist es wichtig dem\n",
            "Team eine Idee vom aktuellen Stand in Bezug auf das Gesamtziel zu geben. Offenheit,\n",
            "gerade in Krisen regt die Empathie an. [1] [2]\n",
            "Bildquellen:\n",
            "•\n",
            "Designed by stories: http://www.freepik.com/\n",
            "4.7 Marketing In-house oder Agentur?\n",
            "Inhouse\n",
            "Agentur\n",
            "Vorteile\n",
            "Nachteile\n",
            "Vorteile\n",
            "Nachteile\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Relevanz-Score: 4.516584396362305\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n",
            "Inhalt:\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Führungspersonal die Kompetenzen einschätzen und verknüpfen kann, da die Kom-\n",
            "munikation zwischen den einzelnen Spaten nicht unbedingt vorausgesetzt ist. Da-\n",
            "durch, dass die Mitarbeiter in den Teams auf einem Kompetenzlevel sind, entsteht in\n",
            "den Spaten ein sehr großes Wissenspotential.\n",
            "4.1 Unternehmen optimieren\n",
            "4.1.4 Unternehmensstruktur\n",
            "108 / 141\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Relevanz-Score: 6.267402172088623\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for doc, score in similar_docs:\n",
        "    print(f\"Inhalt:\\n{'-'*100}\\n{doc.page_content}\")\n",
        "    print(\"-\"*100)\n",
        "    print(f\"Relevanz-Score: {score}\")\n",
        "    print(\"-\"*100 + \"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "579f3f8b",
      "metadata": {
        "tags": [],
        "id": "579f3f8b"
      },
      "source": [
        "Aus der [Modelcard](https://huggingface.co/TheBloke/em_german_mistral_v01-AWQ#example) können wir die Prompt empfehlung für dieses Finegetunte Modell einsehen. Es gibt dabei verschiedene Methoden, aber da das Modell mit einem EM Datensatz gefintuned wurde, wird wahrscheinlich die angegebene Technik am besten Funktionieren. Damit wir sie flexible für verschiedene Versuche verwenden können, bauen wir uns schnell eine kleine Funktion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55cb9d89",
      "metadata": {
        "tags": [],
        "id": "55cb9d89",
        "outputId": "d939970c-988e-4d59-9a29-d4441f338fdd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PromptTemplate(input_variables=['context', 'question'], template='Du bist ein hilfreicher Assistent. Für die folgenden Aufgaben stehen dir zwischen den tags BEGININPUT und ENDINPUT mehrere Quellen zur Verfügung. Metadaten zu den Quellen wie Autor, URL o.ä. sind zwischen BEGINCONTEXT und ENDCONTEXT zu finden, danach folgt der Text der Quellen. Die eigentliche Aufgabe oder Frage ist zwischen BEGININSTRUCTION und ENDINCSTRUCTION zu finden. Extrahiere aus dem Text Informationen die für die Beantwortung der Frage relevant ist! USER: BEGININPUT\\n{context}\\nENDINPUT\\nBEGININSTRUCTION {question} ENDINSTRUCTION\\nASSISTANT:')"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Globale Variablen\n",
        "USER = \"USER:\"\n",
        "ASSISTANT = \"ASSISTANT:\"\n",
        "\n",
        "def generate_retrival_prompt(task_description=\"Extrahiere aus dem Text Informationen die für die Beantwortung der Frage relevant ist!\",\n",
        "                             context_variable_name=\"context\",\n",
        "                             question_variable_name=\"question\",\n",
        "                             plural=True,\n",
        "                             question=None\n",
        "                            ):\n",
        "\n",
        "    if plural:\n",
        "        source_text = \"Quellen\"\n",
        "        article = \"den\"\n",
        "        intro_text = \"Für die folgenden Aufgaben stehen dir zwischen den tags BEGININPUT und ENDINPUT mehrere Quellen zur Verfügung.\"\n",
        "    else:\n",
        "        source_text = \"Quelle\"\n",
        "        article = \"der\"\n",
        "        intro_text = \"Für die folgende Aufgabe steht dir zwischen den tags BEGININPUT und ENDINPUT eine Quelle zur Verfügung.\"\n",
        "\n",
        "    # Automatisches Wrapping von context_variable_name und question_variable_name\n",
        "    if context_variable_name:\n",
        "        context_variable_name = \"{\" + context_variable_name + \"}\"\n",
        "\n",
        "    if not question and question_variable_name:\n",
        "        question = \"{\" + question_variable_name + \"}\"\n",
        "\n",
        "    prompt = f\"\"\"Du bist ein hilfreicher Assistent. {intro_text} Metadaten zu {article} {source_text} wie Autor, URL o.ä. sind zwischen BEGINCONTEXT und ENDCONTEXT zu finden, danach folgt der Text der {source_text}. Die eigentliche Aufgabe oder Frage ist zwischen BEGININSTRUCTION und ENDINCSTRUCTION zu finden. {task_description} {USER} BEGININPUT\n",
        "{context_variable_name or ''}\n",
        "ENDINPUT\n",
        "BEGININSTRUCTION {question or ''} ENDINSTRUCTION\n",
        "{ASSISTANT}\"\"\"\n",
        "\n",
        "    return PromptTemplate.from_template(prompt)\n",
        "\n",
        "# Beispielaufruf:\n",
        "generate_retrival_prompt(plural=True) # plural=true steht dafür, dass wir mehrere Text Chunks mit dieser Prompt verarbeiten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae17e738",
      "metadata": {
        "tags": [],
        "id": "ae17e738",
        "outputId": "fa87d997-9fd6-4a2e-f600-e57a5a6048f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Du bist ein hilfreicher Assistent. Für die folgenden Aufgaben stehen dir zwischen den tags BEGININPUT und ENDINPUT mehrere Quellen zur Verfügung. Metadaten zu den Quellen wie Autor, URL o.ä. sind zwischen BEGINCONTEXT und ENDCONTEXT zu finden, danach folgt der Text der Quellen. Die eigentliche Aufgabe oder Frage ist zwischen BEGININSTRUCTION und ENDINCSTRUCTION zu finden. Extrahiere aus dem Text Informationen die für die Beantwortung der Frage relevant ist! USER: BEGININPUT\n",
            "{context}\n",
            "ENDINPUT\n",
            "BEGININSTRUCTION {question} ENDINSTRUCTION\n",
            "ASSISTANT:\n"
          ]
        }
      ],
      "source": [
        "print(generate_retrival_prompt(plural=True).template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "274f15a0",
      "metadata": {
        "id": "274f15a0"
      },
      "source": [
        "### Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d02258",
      "metadata": {
        "tags": [],
        "id": "e4d02258"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "retrival_prompt = generate_retrival_prompt()\n",
        "\n",
        "chain_type_kwargs = {\"prompt\": retrival_prompt}\n",
        "search_kwargs = {\"k\": 1} # k Text zur Frage passende Text Chunks sollen gesucht werden\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(\n",
        "        search_kwargs=search_kwargs\n",
        "    ),\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c50372bd",
      "metadata": {
        "tags": [],
        "id": "c50372bd",
        "outputId": "7f223dfe-7461-4a5c-813c-b7c4c22dbb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen. [1]"
          ]
        }
      ],
      "source": [
        "answer = qa(\"Was macht ein gutes Team aus?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55b0396c",
      "metadata": {
        "id": "55b0396c"
      },
      "source": [
        "Nehmen wir jetzt an wir haben mehere Text Chunks. In der Prompt Template Vorlage des Modells wird gesagt, wie sollen jedes der Text Chunks wie folgt inbringen:\n",
        "\n",
        "```\n",
        "BEGINCONTEXT\n",
        "Url: https://www.jph.me\n",
        "ENDCONTEXT\n",
        "Das Wetter in Düsseldorf wird heute schön und sonnig!\n",
        "```\n",
        "\n",
        "Dabei steht zwischen den `BEGINKONTEXT` und `ENDCONTEXT` die Matedaten der Quellen und danach der Text. Hierfür können wir ein Dokumenten Template erstellen, welches auf jedes Dokument angwendet wird bevor es in die Prompt geht."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa31e04e",
      "metadata": {
        "tags": [],
        "id": "fa31e04e"
      },
      "outputs": [],
      "source": [
        "# source ist der pdf link in diesem Fall und page_content der inhalt\n",
        "document_prompt = PromptTemplate.from_template(\"BEGINCONTEXT\\nURL:{source}\\nPAGE:{page}\\nENDCONTEXT\\n{page_content}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a90efee",
      "metadata": {
        "tags": [],
        "id": "4a90efee",
        "outputId": "1f576467-a8f6-4664-acc3-d20a5b53e1ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen. [1]"
          ]
        }
      ],
      "source": [
        "chain_type_kwargs = {\n",
        "    \"prompt\": retrival_prompt,\n",
        "    \"document_prompt\": document_prompt\n",
        "}\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=db.as_retriever(\n",
        "        search_kwargs=search_kwargs\n",
        "    ),\n",
        "    chain_type_kwargs=chain_type_kwargs\n",
        ")\n",
        "outputs = qa(\"Was macht ein gutes Team aus?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bcf08ef",
      "metadata": {
        "id": "0bcf08ef"
      },
      "source": [
        "### Map Reduce"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81882f50",
      "metadata": {
        "id": "81882f50"
      },
      "source": [
        "LangChain verfügt über fertige Chains, die mit wenige Befehlen aufgerufen werden können. Jedoch ist darauf zu achten, das im Hintergrund häufig Standard-Prompts verwendet werden. Diese Prompts sind nicht nur auf Englisch, sondern auch meist für OpenAI Modelle ausgelegt.\n",
        "\n",
        "Damit ein verständs aufkommt, wollen wir einmal eine Chain selber zusammen bauen, damit wir wissen wohin welche Prompt eigentlich geht. Dafür verwenden wir das MapReduce Prinzip, welches dem einen oder anderen vielleicht aus Hadoop bekannt ist. Wir reduzieren den Inhalt aus mehreren Quellen vorab mithilfe das Modelles auf einen Text, welchen wir dann wieder in das LLM geben mit unserer Frage:\n",
        "\n",
        "1. Ähnliche Text Chunks suchen\n",
        "2. Wesentlichen Punkte jeder Quelle herausfiltern (mit LLM)\n",
        "3. Wesentliche Punkte der einzelnen Quellen zu einem neuen Text kombinieren (mit LLM)\n",
        "4. Kombinierte Informationen mit der Frage an LLM übergeben um eine Antwort zu erhalten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ae53ef9",
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "c781843ca1b641128aa8d2ecd13a66bc",
            "486090d71a8c4050a4073352b596c8f1",
            "ff08a3e74a28455f8990564be3f8ed39",
            "1744559022164a43be3c9a6ebac25076"
          ]
        },
        "id": "8ae53ef9",
        "outputId": "0ddd9996-978b-4a63-f252-66d84bc41141"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen. [1] Jedes Teammitglied sollte als Individuum mit eigenen Stärken und Schwächen behandelt werden. Das sorgt nicht nur für eine bessere kreative Entfaltung, sondern auch für wachsende Motivation. Mit den gegebenen Informationen ist diese Frage nicht zu beantworten."
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c781843ca1b641128aa8d2ecd13a66bc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "486090d71a8c4050a4073352b596c8f1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff08a3e74a28455f8990564be3f8ed39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1744559022164a43be3c9a6ebac25076",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen."
          ]
        }
      ],
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "\n",
        "# Extrahiert information aus einzelnen Chunks jeweils eine Zusammenfassung\n",
        "question_prompt = generate_retrival_prompt(plural=True)\n",
        "\n",
        "# Kombiniert Zusammenfassungen\n",
        "combine_prompt = generate_retrival_prompt(\n",
        "    task_description=\"Kombiniere die Informationen aus den unterschiedlichen Textausschnitten zu einem kurzem Text der alle wesentliche Informationen hinsichtlich der Fragen beinhaltet. Sollten diese keine Antwort enthalten, antworte, dass auf Basis der gegebenen Informationen keine Antwort möglich ist!\",\n",
        "    context_variable_name=\"summaries\"\n",
        ")\n",
        "\n",
        "# Besten drei Ergebnisse mit dem jeweiligen Score\n",
        "query = \"Was macht ein gutes Team aus\"\n",
        "similar_docs = db.similarity_search(query, k=3)\n",
        "\n",
        "# Vorbereiten der Chain mit dem prompts\n",
        "chain = load_qa_chain(\n",
        "    llm,\n",
        "    chain_type=\"map_reduce\",\n",
        "    return_map_steps=True,\n",
        "    question_prompt=question_prompt,\n",
        "    combine_prompt=combine_prompt\n",
        ")\n",
        "outputs = chain({\"input_documents\": similar_docs, \"question\": query}, return_only_outputs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e21aadc",
      "metadata": {
        "tags": [],
        "id": "6e21aadc",
        "outputId": "40783bed-c93b-4b55-ca71-ea5e2aabbe16"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[' Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen. [1]',\n",
              " ' Jedes Teammitglied sollte als Individuum mit eigenen Stärken und Schwächen behandelt werden. Das sorgt nicht nur für eine bessere kreative Entfaltung, sondern auch für wachsende Motivation.',\n",
              " ' Mit den gegebenen Informationen ist diese Frage nicht zu beantworten.']"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs[\"intermediate_steps\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99bfddee",
      "metadata": {
        "id": "99bfddee"
      },
      "source": [
        "----\n",
        "Die Methode mit load_qa_chain zu Arbeiten ist nichts anders als auch das was im Code von RetrievalQA passiert. Einzige unterschied ist, dass die Logik des Retrieval hier fehlt, demnach mussten wir uns die Text Chunks wieder selber aus dem Vectorestore holen. Den Vorteil hat diese Methode aber, da sie die Rückgabe von 'intermediate_steps' erlaubt. Das bedeutet wir können sehen, was das Modell aus den jeweiligen Text Chunks extrahiert hat, demnach die Schritte zur finalen Antwort. Dies kann manchmal von Vorteil sein."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac64f49a",
      "metadata": {
        "id": "ac64f49a"
      },
      "source": [
        "-----\n",
        "Jetzt implementieren wir aber lieber wieder die Classe `RetrievalQA`, aber diesmal nehmen wir eine besondere Version, die `RetrievalQAWithSourcesChain`, die es uns auch ermöglich die Text Chunks samt ihrer Metadaten mit zu erhalten. Dies ermöglicht es uns die Antwort auf ihre Urpsrung zuzurück zu verfolgen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cd8b309",
      "metadata": {
        "tags": [],
        "id": "4cd8b309",
        "outputId": "ddf951b4-bce2-493b-98e9-4e87ddda97b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen. [1] Jedes Teammitglied sollte als Individuum mit eigenen Stärken und Schwächen behandelt werden. Das sorgt nicht nur für eine bessere kreative Entfaltung, sondern auch für wachsende Motivation. Mit den gegebenen Informationen ist diese Frage nicht zu beantworten. Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen."
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "chain_type_kwargs = {\n",
        "    \"document_prompt\": document_prompt,\n",
        "    \"question_prompt\": question_prompt,\n",
        "    \"combine_prompt\": combine_prompt\n",
        "}\n",
        "\n",
        "search_kwargs[\"k\"] = 3\n",
        "\n",
        "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"map_reduce\",\n",
        "    retriever=db.as_retriever(\n",
        "        search_kwargs=search_kwargs\n",
        "    ),\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        "    return_source_documents=True\n",
        ")\n",
        "outputs = qa(\"Was macht ein gutes Team aus?\", return_only_outputs=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39a03605",
      "metadata": {
        "id": "39a03605"
      },
      "source": [
        "### Output in Markdown Example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d28db80",
      "metadata": {
        "id": "3d28db80"
      },
      "source": [
        "Den Output können wir jetzt Visuell aufbereiten und beispielsweise in eine App verbauen. Hier ein kurzes Markdown Beispiel mit einer Interaktiven Input Zelle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdf82c3e",
      "metadata": {
        "tags": [],
        "id": "bdf82c3e",
        "outputId": "a46d9705-0a7e-4d0b-e4b3-1dfabf7cec58"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'**Frage:** Was macht ein gutes Team aus?\\n\\n**Antwort:**  Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. \\n- Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. \\n- Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen.\\n\\n**Quelldokumente:**\\n\\n- **Titel:** Gründungswiki | **Seite:** 120 von 141\\n  - **Inhalt:** •\\nEin erfolgreiches Team kommu-\\nniziert die Ziele und den Weg\\ndahin ausführlich und regel-\\nmäßig, damit jedes Mitglied auf\\nden aktuellen Stand ist. Ein er-\\nfolgreiches Team findet effek-\\ntive und effiziente Entscheidun-\\ngen, die die Ausgangssituation\\nkurz- oder langfristig verbessert.\\n•\\nEin erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus\\nFehlern gelernt wird und Fortschritt entsteht.\\n•\\nEin erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß\\nsie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst.\\nDiversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem\\nentstehen. [1]\\nPraktische Tipps, um ein Team aufzubauen\\nZiele klar definieren\\nDas beste Team bringt nichts, wenn\\n- **Titel:** Gründungswiki | **Seite:** 121 von 141\\n  - **Inhalt:** sätze nicht zu umständlich zu formulieren. Es muss möglich sein aus ihnen in bes-\\ntimmten Situation abzuleiten, wie zu handeln ist.\\nDer Umgang\\nJedes Teammitglied sollte als Individuum mit eigenen Stärken und Schwächen behan-\\ndelt werden. Das sorgt nicht nur für eine bessere kreative Entfaltung, sondern auch für\\nwachsende Motivation.\\nDie Kommunikation\\nAls Teamleader sollte nicht immer von oben herab entschieden werden, auch wenn\\nsich das manchmal nicht vermeiden lässt. In diesen Situationen ist es wichtig dem\\nTeam eine Idee vom aktuellen Stand in Bezug auf das Gesamtziel zu geben. Offenheit,\\ngerade in Krisen regt die Empathie an. [1] [2]\\nBildquellen:\\n•\\nDesigned by stories: http://www.freepik.com/\\n4.7 Marketing In-house oder Agentur?\\nInhouse\\nAgentur\\nVorteile\\nNachteile\\nVorteile\\nNachteile\\n- **Titel:** Gründungswiki | **Seite:** 107 von 141\\n  - **Inhalt:** Führungspersonal die Kompetenzen einschätzen und verknüpfen kann, da die Kom-\\nmunikation zwischen den einzelnen Spaten nicht unbedingt vorausgesetzt ist. Da-\\ndurch, dass die Mitarbeiter in den Teams auf einem Kompetenzlevel sind, entsteht in\\nden Spaten ein sehr großes Wissenspotential.\\n4.1 Unternehmen optimieren\\n4.1.4 Unternehmensstruktur\\n108 / 141\\n'"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def pretty_print_to_markdown(data):\n",
        "    output = \"\"\n",
        "\n",
        "    # Frage und Antwort\n",
        "    output += f\"**Frage:** {data['question']}\\n\\n\"\n",
        "    replace_string = '• '\n",
        "    replacement = '\\n- '\n",
        "    output += \"**Antwort:** \" + data['answer'].replace(replace_string, replacement) + \"\\n\\n\"\n",
        "\n",
        "\n",
        "\n",
        "    # Quelldokumente\n",
        "    if data['source_documents']:\n",
        "        output += \"**Quelldokumente:**\\n\\n\"\n",
        "        for doc in data['source_documents']:\n",
        "            metadata = doc.metadata\n",
        "            title = metadata.get('title', 'Unbekannter Titel')\n",
        "            page = metadata.get('page', 'Unbekannte Seite')\n",
        "            total_pages = metadata.get('total_pages', '')\n",
        "            output += f\"- **Titel:** {title} | **Seite:** {page} von {total_pages}\\n\"\n",
        "            output += f\"  - **Inhalt:** {doc.page_content}\\n\"\n",
        "\n",
        "    return output\n",
        "\n",
        "pretty_print_to_markdown(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed13c58e",
      "metadata": {
        "tags": [],
        "colab": {
          "referenced_widgets": [
            "85ca276fea194096909307e9cb2aeea5",
            "6d0367d6df654594a4f2920c76d04f97"
          ]
        },
        "id": "ed13c58e",
        "outputId": "03f1103a-6b8d-4c45-a53c-d77d0b40d32b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85ca276fea194096909307e9cb2aeea5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Text(value='', continuous_update=False, description='Frage:', placeholder='Geb hier eure Frage ein...')"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d0367d6df654594a4f2920c76d04f97",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Output()"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Interaktives Modul\n",
        "def interactive_qa_module():\n",
        "    # Widget-Definitionen\n",
        "    question_input = widgets.Text(\n",
        "        value='',\n",
        "        placeholder='Geb hier eure Frage ein...',\n",
        "        description='Frage:',\n",
        "        disabled=False,\n",
        "        continuous_update=False  # Aktion wird nur nach Drücken der Eingabetaste ausgelöst\n",
        "    )\n",
        "\n",
        "    output_area = widgets.Output()\n",
        "\n",
        "    # Funktion, die aufgerufen wird, wenn der Wert geändert wird (nach Drücken der Eingabetaste)\n",
        "    def on_value_change(change):\n",
        "        with output_area:\n",
        "            # Daten von der LLM Chain erhalten\n",
        "            data = qa(change['new'])\n",
        "\n",
        "            # Markdown-Ausgabe erstellen\n",
        "            markdown_output = pretty_print_to_markdown(data)\n",
        "\n",
        "            # Vorherigen Output löschen\n",
        "            output_area.clear_output(wait=True)\n",
        "\n",
        "            # Markdown ausgeben\n",
        "            display(Markdown(markdown_output))\n",
        "\n",
        "    question_input.observe(on_value_change, names='value')\n",
        "\n",
        "    # Widgets anzeigen\n",
        "    display(question_input, output_area)\n",
        "\n",
        "# Die interaktive Funktion aufrufen\n",
        "interactive_qa_module()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fcb437",
      "metadata": {
        "id": "06fcb437"
      },
      "source": [
        "### ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5916a7bd",
      "metadata": {
        "tags": [],
        "id": "5916a7bd"
      },
      "outputs": [],
      "source": [
        "from langchain.chains.qa_with_sources import load_qa_with_sources_chain\n",
        "from langchain.chains import LLMChain, ConversationalRetrievalChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57587897",
      "metadata": {
        "tags": [],
        "id": "57587897"
      },
      "outputs": [],
      "source": [
        "condense_question_prompt = PromptTemplate.from_template(\"\"\"Du bist ein hilfreicher Assistent. Formuliere die folgende Unterhaltung und eine Folgefrage so um, dass sie eine eigenständige Frage ist.\\n\\nChat-Verlauf:\\n{chat_history}\\nFolgefrage: {question}\\nStandalone question:\"\"\")\n",
        "\n",
        "question_generator = LLMChain(llm=llm, prompt=condense_question_prompt)\n",
        "doc_chain = load_qa_with_sources_chain(\n",
        "    llm,\n",
        "    chain_type=\"map_reduce\",\n",
        "    question_prompt=question_prompt,\n",
        "    combine_prompt=combine_prompt\n",
        ")\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    human_prefix=\"USER\",\n",
        "    ai_prefix=\"ASSISTANT\",\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True\n",
        ")\n",
        "\n",
        "chain = ConversationalRetrievalChain(\n",
        "    memory=memory,\n",
        "    retriever=db.as_retriever(),\n",
        "    question_generator=question_generator,\n",
        "    combine_docs_chain=doc_chain,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9ca06f9",
      "metadata": {
        "tags": [],
        "id": "e9ca06f9",
        "outputId": "e76f103e-a925-4924-e3c5-037071cacba4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen. [1] Jedes Teammitglied sollte als Individuum mit eigenen Stärken und Schwächen behandelt werden. Das sorgt nicht nur für eine bessere kreative Entfaltung, sondern auch für wachsende Motivation. Ich bin überzeugt, dass ein Team unglaublich wichtig ist. Da kann die Idee noch\n",
            "so gut sein, wenn das Team nicht passt, klappt es nicht. Mit den gegebenen Informationen ist diese Frage nicht zu beantworten. Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen."
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Was macht ein gutes Team?',\n",
              " 'chat_history': [HumanMessage(content='Was macht ein gutes Team?'),\n",
              "  AIMessage(content=' Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen.')],\n",
              " 'answer': ' Ein erfolgreiches Team kommuniziert die Ziele und den Weg dahin ausführlich und regelmäßig, damit jedes Mitglied auf den aktuellen Stand ist. Ein erfolgreiches Team findet effektive und effiziente Entscheidungen, die die Ausgangssituation kurz- oder langfristig verbessern. • Ein erfolgreiches Team verfügt über eine gesunde Fehlerkultur und weiß, dass aus Fehlern gelernt wird und Fortschritt entsteht. • Ein erfolgreiches Team kennt die individuellen Stärken und Schwächen und weiß sie zu schätzen. Dadurch werden Aufgaben effizient verteilt und Probleme gelöst. Diversität sorgt zudem dafür, dass mehrere Sichtweisen auf das gleiche Problem entstehen.'}"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_history = []\n",
        "query = \"Was macht ein gutes Team?\"\n",
        "chain({\"question\": query, \"chat_history\": chat_history})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e708290",
      "metadata": {
        "tags": [],
        "id": "3e708290"
      },
      "source": [
        "Auf diese Weise können wir einen Chatverlauf mit die Chain geben. Dadurch kann Beispielsweise ein Chatverlauf geführt werden und bei Bedarf kann die Retrival Chain benutzt werden, um die Frage das Users zu beantworten."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94635050",
      "metadata": {
        "id": "94635050"
      },
      "source": [
        "## Web Scraping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abf74bc2",
      "metadata": {
        "tags": [],
        "id": "abf74bc2",
        "outputId": "2e487554-a380-48b3-dfe3-ad461acba0d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0:\n",
            "  Metadaten:{'Header': '1. Persönliche Hochschule'}\n",
            "  Page_content:Foto: TH Lübeck  \n",
            "In Lübeck bist du mehr als eine Nummer! Hier lernst du in kleinen Seminargruppen und profitierst durch eine enge Verbindung zu den Professor*innen. Viele von ihnen werden dich innerhalb kürzester Zeit beim Namen kennen, deine Entwicklung im Blick haben und all unsere Dozierenden stehen gerne für Fragen, Konsultationen und Sprechstunden zur Verfügung. Außerdem ist der Campus, also das Gelände unserer Hochschule, an einem Ort. Das bedeutet für dich, dass du nicht zwischen den Vorlesungen durch die ganze Stadt fahren musst. Stattdessen kannst du die Zeit nutzen, um dich mit deinen Lerngruppen zu treffen oder mit Freunden im Carlebach-Park direkt am Campus zu entspannen.\n",
            "1:\n",
            "  Metadaten:{'Header': '2. Hoher Praxisbezug'}\n",
            "  Page_content:Kennst du den Unterschied zwischen einer Universität und einer Fachhochschule? Qualitativ sind diese beiden Hochschul-Formen gleichwertig, aber sie haben unterschiedliche Ziele: Eine Universität bereitet dich eher auf eine Karriere als Wissenschaftler*in vor, während dich eine Fachhochschule wie die TH Lübeck für die Wirtschaft fit macht. Das bedeutet, dass die Professor*innen an der TH Lübeck alle schon mal selbst in der Wirtschaft gearbeitet haben. Dadurch sind sie in der Lage, dich passgenau auf das Arbeitsleben in einem Unternehmen vorzubereiten. Nicht zuletzt deshalb wirst du in deinem FH-Studium auch nicht nur graue Theorie lernen. Greifbare Praxisprojekte – oft auch direkt mit Unternehmen – ebnen dir den Weg zu deiner Karriere. An der TH Lübeck kannst du außerdem auch gleich im\n",
            "2:\n",
            "  Metadaten:{'Header': '2. Hoher Praxisbezug'}\n",
            "  Page_content:du außerdem auch gleich im Studium wertvolle Zusatzqualifikationen erlangen, die sonst viel Geld kosten würden. Aus diesen Gründen verdienen Fachhochschul-Absolvent*innen im Schnitt auch mehr in ihrem ersten Job als Uni-Absolvent*innen.\n",
            "3:\n",
            "  Metadaten:{'Header': '3. Internationale Angebote'}\n",
            "  Page_content:Foto: TH Lübeck  \n",
            "Die Welt wird zunehmend internationaler – sowohl in der Wirtschaft, in der Wissenschaft als auch im Privatleben. Die TH Lübeck hat eine Strategie, diese Internationalisierung für dich nutzbar zu machen. So hast du zum Beispiel die Möglichkeit, Doppelabschlüsse mit ausländischen Hochschulen abzuschließen, kannst kostenfreie Sprachkurse belegen und bekommst viel Unterstützung bei der Planung deines Auslandssemesters oder -praktikums.\n",
            "4:\n",
            "  Metadaten:{'Header': '4. Engagierte Studierende'}\n",
            "  Page_content:Foto: TH Lübeck  \n",
            "Studium bedeutet mehr als Lernen! Die Studienzeit gilt traditionell als eine Zeit, in der man sich besonders intensiv selbst verwirklichen kann. An der TH Lübeck findest du viele Studierende, die sich in Gremien an der Entwicklung der Hochschule beteiligen, sich sozial engagieren und Initiativen starten, die die Welt ein Stückchen besser, interessanter, lehrreicher oder unterhaltsamer machen. Wenn du selbst ein Interesse daran hast, deine Welt mitzugestalten, dann triffst du an der TH Lübeck auf viele Gleichgesinnte. Beteilige dich beispielsweise bei den Students for Sustainability, engagiere dich in der Campus-Imkerei oder baue mit den „Seagulls“ einen eigenen Rennwagen.\n",
            "5:\n",
            "  Metadaten:{'Header': '5. Attraktive Stadt'}\n",
            "  Page_content:Foto: TH Lübeck  \n",
            "Unsere Studierenden wissen insbesondere den hohen Freizeitwert Lübecks und der Region zu schätzen. Durch die Nähe zur Ostsee kannst du ein umfangreiches Angebot an kulturellen und sportlichen Möglichkeiten genießen. Trotz der relativ geringen Anzahl der Studierenden an der Gesamteinwohnerzahl ist das Stadtbild studentisch geprägt. Mieten und andere Lebenshaltungskosten sind im Vergleich zu anderen Hochschulstädten moderat. Nach einem langen Studientag oder einem Strandaufenthalt kannst du in gemütlichen Altstadtkneipen, Bars, Theatern und Kinos den Tag ausklingen oder die Nacht zum Tag werden lassen. Wem das alles nicht reicht, der findet in unmittelbarer Nachbarschaft die Metropole Hamburg.\n",
            "6:\n",
            "  Metadaten:{'Header': 'Das könnte dich auch interessieren'}\n",
            "  Page_content:Das Studienangebot der TH Lübeck im Überblick  \n",
            "Online studieren an der TH Lübeck  \n",
            "StudiLe - Studium mit integrierter Lehre  \n",
            "Erste Schritte im Studium\n"
          ]
        }
      ],
      "source": [
        "from langchain.text_splitter import HTMLHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
        "import requests\n",
        "\n",
        "TH_URL = \"https://www.th-luebeck.de/studium/studienorientierung/warum-th-luebeck/\"\n",
        "\n",
        "# Wir können nach HTML Elemente splitten\n",
        "headers_to_split_on = [\n",
        "    (\"h2\", \"Header\"),\n",
        "]\n",
        "\n",
        "# Hier werden Textblöcke nach den h2 Überschriften gesplittet\n",
        "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
        "\n",
        "# Text Chunks von Webseite ziehen.\n",
        "html_header_splits = html_splitter.split_text_from_url(TH_URL)\n",
        "\n",
        "# Alternative Textsplitting Einstellung, falls h2 nich konsistent genug war.\n",
        "chunk_size = 800\n",
        "chunk_overlap = 30\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
        ")\n",
        "\n",
        "# Split anwenden\n",
        "splits = text_splitter.split_documents(html_header_splits)\n",
        "\n",
        "# Filter für Abschnitte die eine Überschrift haben und mindestens 30 Zeichen lang sind\n",
        "splits = [split for split in splits if len(split.page_content) > 30 and \"Header\" in split.metadata]\n",
        "for i, split in enumerate(splits):\n",
        "    print(f\"{i}:\\n  Metadaten:{split.metadata}\\n  Page_content:{split.page_content}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e430b5bf",
      "metadata": {
        "tags": [],
        "id": "e430b5bf"
      },
      "outputs": [],
      "source": [
        "# Chroma Vectorstore vorbereiten\n",
        "if db._client.list_collections():\n",
        "    db.delete_collection()\n",
        "db = Chroma.from_documents(splits, embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9938552f",
      "metadata": {
        "tags": [],
        "id": "9938552f",
        "outputId": "368c64d2-067f-4888-c805-922587d92a0e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Die TH Lübeck hat eine Strategie, diese Internationalisierung für dich nutzbar zu machen. So hast du zum Beispiel die Möglichkeit, Doppelabschlüsse mit ausländischen Hochschulen abzuschließen, kannst kostenfreie Sprachkurse belegen und bekommst viel Unterstützung bei der Planung deines Auslandssemesters oder -praktikums. Mit den Quellen, die bereitgestellt wurden, ist diese Frage nicht zu beantworten. Mit den Quellen, die bereitgestellt wurden, ist diese Frage nicht zu beantworten. Die TH Lübeck hat eine Strategie, diese Internationalisierung für dich nutzbar zu machen. So hast du zum Beispiel die Möglichkeit, Doppelabschlüsse mit ausländischen Hochschulen abzuschließen, kannst kostenfreie Sprachkurse belegen und bekommst viel Unterstützung bei der Planung deines Auslandssemesters oder -praktikums.\n",
            "Antwort:\n",
            " Die TH Lübeck hat eine Strategie, diese Internationalisierung für dich nutzbar zu machen. So hast du zum Beispiel die Möglichkeit, Doppelabschlüsse mit ausländischen Hochschulen abzuschließen, kannst kostenfreie Sprachkurse belegen und bekommst viel Unterstützung bei der Planung deines Auslandssemesters oder -praktikums.\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import RetrievalQAWithSourcesChain\n",
        "\n",
        "document_prompt = PromptTemplate.from_template(\"BEGINCONTEXT\\nHEADER:{Header}\\nENDCONTEXT\\n{page_content}\\n\")\n",
        "\n",
        "chain_type_kwargs = {\n",
        "    \"document_prompt\": document_prompt,\n",
        "    \"question_prompt\": question_prompt,\n",
        "    \"combine_prompt\": combine_prompt\n",
        "}\n",
        "\n",
        "search_kwargs[\"k\"] = 3\n",
        "\n",
        "qa = RetrievalQAWithSourcesChain.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"map_reduce\",\n",
        "    retriever=db.as_retriever(\n",
        "        search_kwargs=search_kwargs\n",
        "    ),\n",
        "    chain_type_kwargs=chain_type_kwargs,\n",
        "    return_source_documents=True\n",
        ")\n",
        "outputs = qa(\"Was hat die Technische Hochschule Lübeck für internationale Angebote?\", return_only_outputs=False)\n",
        "print(f\"\\nAntwort:\\n{outputs['answer']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80e3f569",
      "metadata": {
        "tags": [],
        "id": "80e3f569"
      },
      "source": [
        "# Agents und Tools"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c61005a9-88a8-4342-ac5c-023090b90d06",
      "metadata": {
        "id": "c61005a9-88a8-4342-ac5c-023090b90d06"
      },
      "source": [
        "# Kapitel: Agenten & Werkzeuge - Das dynamische Duo 🦸‍♂️🦸‍♀️\n",
        "\n",
        "Nun, da ihr bereits mit LangChain vertraut seid, wollen wir einen Schritt weiter gehen. Es ist Zeit, den zwei entscheidenden Akteuren auf der Bühne Raum zu geben: den `Agents` und den `Tools`. Sie sind wie Batman und Robin der LangChain-Welt.\n",
        "\n",
        "**Agenten** sind unsere Entscheidungstreiber, die basierend auf den Benutzereingaben den besten Kurs festlegen. **Werkzeuge** hingegen sind die Ressourcen, mit denen unsere Agenten interagieren, um ihre Ziele zu erreichen.\n",
        "\n",
        "Zum Aufwärmen bauen wir gemeinsam ein einfaches, aber mächtiges Tool. Stellt euch vor, ihr hättet einen Taschenrechner, der nicht nur rechnet, sondern auch versteht! Unser erstes Tool wird mathematische Ausdrücke interpretieren und mithilfe der Python `numexpr`-Bibliothek berechnen.\n",
        "\n",
        "Bereit? Lasst uns mit unserem Taschenrechner-Werkzeug beginnen! 🔢🛠️\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "029d3108",
      "metadata": {
        "tags": [],
        "id": "029d3108"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "USER = \"USER\"\n",
        "ASSISTANT = \"ASSISTANT\"\n",
        "\n",
        "memory = ConversationBufferMemory(\n",
        "    human_prefix=USER,\n",
        "    ai_prefix=ASSISTANT,\n",
        "    return_messages=True,\n",
        "    memory_key=\"chat_history\",\n",
        "    k=5,\n",
        "    output_key=\"output\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7b2133c-5ce1-4f5b-a49a-e040003d98b7",
      "metadata": {
        "id": "f7b2133c-5ce1-4f5b-a49a-e040003d98b7"
      },
      "source": [
        "In Langchain sind bereits verschiedene `Tools` [integriert](https://python.langchain.com/docs/integrations/tools). Diese beinhalten Prompts, die den LLM detailliert anleiten, wie sie jedes Tool nutzen können und welche Tools verfügbar sind. Das ist natürlich hervorragend, aber die Hürde: Sie sind primär für OpenAI und die englische Sprache konzipiert. Daher ist es notwendig, die Prompts in jedem Schritt anzupassen. So stellen wir sicher, dass unser LLM flüssig bleibt, nicht ins Englische abdriftet und konsequent `USER` und `ASSISTANT` verwendet.\n",
        "\n",
        "Lasst uns zuerst die Originale Prompt anschauen:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba71eff7-13c6-483c-b70e-52f969c195e5",
      "metadata": {
        "tags": [],
        "id": "ba71eff7-13c6-483c-b70e-52f969c195e5",
        "outputId": "a13181fc-c7ca-426a-c86e-3e44f9f61c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Translate a math problem into a expression that can be executed using Python's numexpr library. Use the output of running this code to answer the question.\n",
            "\n",
            "Question: ${{Question with math problem.}}\n",
            "```text\n",
            "${{single line mathematical expression that solves the problem}}\n",
            "```\n",
            "...numexpr.evaluate(text)...\n",
            "```output\n",
            "${{Output of running the code}}\n",
            "```\n",
            "Answer: ${{Answer}}\n",
            "\n",
            "Begin.\n",
            "\n",
            "Question: What is 37593 * 67?\n",
            "```text\n",
            "37593 * 67\n",
            "```\n",
            "...numexpr.evaluate(\"37593 * 67\")...\n",
            "```output\n",
            "2518731\n",
            "```\n",
            "Answer: 2518731\n",
            "\n",
            "Question: 37593^(1/5)\n",
            "```text\n",
            "37593**(1/5)\n",
            "```\n",
            "...numexpr.evaluate(\"37593**(1/5)\")...\n",
            "```output\n",
            "8.222831614237718\n",
            "```\n",
            "Answer: 8.222831614237718\n",
            "\n",
            "Question: {question}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains.llm_math.prompt import PROMPT as math_chain_prompt\n",
        "\n",
        "print(math_chain_prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf433938-59d1-491b-bf66-5db67efe299b",
      "metadata": {
        "id": "bf433938-59d1-491b-bf66-5db67efe299b"
      },
      "source": [
        "Wir können die Übersetzung davon fast komplett als Prompt für uns übernehmen. Zusätzlich können wir mit [Pydantic](https://docs.pydantic.dev/latest/):\n",
        "\n",
        "1. **Sicherheit** gewährleisten, indem fehlerhaften oder schädlichen Input verhindert wird.\n",
        "2. **Konsistenz** in unseren Daten sicherstellen, indem sichergestellt wird, dass der Input den erwarteten Formaten und Standards entspricht.\n",
        "3. **Effizienz** steigern, indem fehlerhafte Eingaben frühzeitig erkannt werden, was unnötige Verarbeitungszyklen spart.\n",
        "\n",
        "Durch den Einsatz von Pydantic können wir also nicht nur die Integrität unserer Daten sicherstellen, sondern auch die Gesamtqualität und Effizienz unseres Systems optimieren."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83ae483d",
      "metadata": {
        "tags": [],
        "id": "83ae483d"
      },
      "outputs": [],
      "source": [
        "from pydantic import BaseModel, Field\n",
        "from langchain.chains import LLMMathChain\n",
        "from langchain.tools import Tool\n",
        "\n",
        "math_prompt_template = \"\"\"Übersetze ein mathematisches Problem in einen Ausdruck, der mit der numexpr-Bibliothek von Python ausgeführt werden kann. Verwende die Ausgabe des ausgeführten Codes, um die Frage zu beantworten.\n",
        "\n",
        "Question: ${{Frage mit mathematischem Problem.}}\n",
        "\n",
        "```text\n",
        "${{einzelliger mathematischer Ausdruck zur Lösung des Problems}}\n",
        "```\n",
        "...numexpr.evaluate(text)...\n",
        "```output\n",
        "${{Ausgabe des ausgeführten Codes}}\n",
        "```\n",
        "Answer: ${{Antwort}}\n",
        "\n",
        "Begin.\n",
        "\n",
        "Question: Was ergibt 4 hoch 10?\n",
        "\n",
        "```\n",
        "4**10\n",
        "```\n",
        "...numexpr.evaluate(\"4**10\")...\n",
        "```output\n",
        "1048576\n",
        "```\n",
        "Answer: 1048576\n",
        "\n",
        "Question: 37593^(1/5)\n",
        "```text\n",
        "37593**(1/5)\n",
        "```\n",
        "...numexpr.evaluate(\"37593**(1/5)\")...\n",
        "```output\n",
        "8.222831614237718\n",
        "```\n",
        "Answer: 8.222831614237718\n",
        "Question: {question}\"\"\"\n",
        "\n",
        "# Erstellen der Mathe Chain mit unserer Custom Prompt und unserem LLM\n",
        "math_prompt = PromptTemplate.from_template(math_prompt_template)\n",
        "llm_math_chain = LLMMathChain.from_llm(\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    prompt=math_prompt\n",
        ")\n",
        "\n",
        "# Pydantic Model zum Festlegen des Inputformats das `Calculator`\n",
        "class CalculatorInput(BaseModel):\n",
        "    question: str = Field()\n",
        "\n",
        "# Vorbereiten der Tool Liste für unseren Agent\n",
        "tools = [\n",
        "    Tool.from_function(\n",
        "        func=llm_math_chain.run,\n",
        "        name=\"Rechner\",\n",
        "        description=\"nützlich, wenn Sie Fragen zur Mathematik beantworten müssen\",\n",
        "        args_schema=CalculatorInput\n",
        "        # coroutine= ... <- hier könnt ihr bei Bedarf auch eine asynchrone Methode angeben\n",
        "    )\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db44f376",
      "metadata": {
        "tags": [],
        "id": "db44f376",
        "outputId": "7564566b-d109-4e1d-a9ef-d9d4812c1cb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "Was ist 3.1 hoch 2?\n",
            "```text\n",
            "3.1**2\n",
            "```\n",
            "...numexpr.evaluate(\"3.1**2\")...\n",
            "```output\u001b[32;1m\u001b[1;3m\n",
            "```text\n",
            "3.1**2\n",
            "```\n",
            "...numexpr.evaluate(\"3.1**2\")...\n",
            "```output\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m9.610000000000001\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'question': 'Was ist 3.1 hoch 2?', 'answer': 'Answer: 9.610000000000001'}"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_math_chain(\"Was ist 3.1 hoch 2?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bd4b3f7-2034-4249-bc61-693975970ada",
      "metadata": {
        "id": "3bd4b3f7-2034-4249-bc61-693975970ada"
      },
      "source": [
        "Der Rechner funktioniert schonmal. Später verwenden wir diese Chain als `Tool` in unserer `Agent`-Chain, sodass unser Agent das Rechnertool verwenden kann wenn er meint, dass es bei der Frage nützlich ist."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae7dcc9-9e9e-40fd-8e1f-df2a985cd45c",
      "metadata": {
        "id": "4ae7dcc9-9e9e-40fd-8e1f-df2a985cd45c"
      },
      "source": [
        "## Custom Parser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41b7d45-ba6d-4ddb-abf0-a6c2e274df06",
      "metadata": {
        "id": "e41b7d45-ba6d-4ddb-abf0-a6c2e274df06"
      },
      "source": [
        "Unser `Tool` ist nichts anderes als eine Python Funktion oder irgendeine Art von API. Damit das `LLM` die Frage des `Users` interpretiert und ein Tool verwendet, müssen wir dem `Agent` erklären, in welchen Format er ein Tool verwenden kann. Dies ist besonders wichtig, da das Format der Anweisung nur übertragen werden kann, wenn es genau den Formatierungsforschriften entspricht. Wir müssen dann aus dem Text die einzelnen Elemente extrahieren und die vom `Agent` ausgewählte `Action`ausführen. Dafür können wir einen Custom `OutputParser` verwenden. Dieser wendelt den Textblook in tatsächliche `Action` und `Action-Input` um und leitet diese dann über die Klasse `AgentAction` weiter an das jeweilige Tool.\n",
        "\n",
        "Wichtig bei diesem Konzept ist, dass wir das vorlagerte LLM glauben lassen, dass der User das Tool ausführt. Es wurde auf Konversationen trainiert, weshalb wir so tun als wenn es uns die Anweisung als Json gibt und der User die Funktion ausführt und des Ergebnis als \"Chatnachricht\" zurück in die Konversation gibt. In der Realität werden jedoch andere LLMChains und Python Funktionen dafür automatisch verwendet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "897a1c3f",
      "metadata": {
        "tags": [],
        "id": "897a1c3f"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import AgentOutputParser\n",
        "from langchain.output_parsers.json import parse_json_markdown\n",
        "from langchain.schema import AgentAction, AgentFinish\n",
        "\n",
        "FORMAT_INSTRUCTIONS = \"\"\"ANWEISUNGEN FÜR DAS ANTWORTFORMAT\n",
        "----------------------------\n",
        "\n",
        "Wenn du mir antwortest, gebe bitte eine Antwort in einem von zwei Formaten aus:\n",
        "\n",
        "**Option 1:**\n",
        "Verwende diese, wenn du möchtest, dass der Nutzer ein Tool verwendet.\n",
        "Markdown-Codeausschnitt im folgenden Schema formatiert:\n",
        "\n",
        "```json\n",
        "{{{{\n",
        "\"action\": string, \\\\\\\\ Die zu ergreifende Aktion. Muss eine von {tool_names}\n",
        "\"action_input\": string \\\\\\\\ Die Eingabe für die Aktion\n",
        "}}}}\n",
        "```\n",
        "**Option #2:**\n",
        "Verwende diese, wenn du direkt auf den Menschen reagieren möchten. Markdown-Code-Schnipsel, formatiert nach dem folgenden Schema:\n",
        "```json\n",
        "{{{{\n",
        "\"action\": \"Final Answer\",\n",
        "\"action_input\": string \\\\\\\\ Hier sollten Sie eingeben, was Sie zurückgeben möchten\n",
        "}}}}\n",
        "```\"\"\"\n",
        "\n",
        "class OutputParser(AgentOutputParser):\n",
        "    def get_format_instructions(self) -> str:\n",
        "        return FORMAT_INSTRUCTIONS\n",
        "\n",
        "    def parse(self, text: str) -> AgentAction | AgentFinish:\n",
        "        try:\n",
        "            # this will work IF the text is a valid JSON with action and action_input\n",
        "            response = parse_json_markdown(text)\n",
        "            action, action_input = response[\"action\"], response[\"action_input\"]\n",
        "            if action == \"Final Answer\":\n",
        "                # this means the agent is finished so we call AgentFinish\n",
        "                return AgentFinish({\"output\": action_input}, text)\n",
        "            else:\n",
        "                # otherwise the agent wants to use an action, so we call AgentAction\n",
        "                return AgentAction(action, action_input, text)\n",
        "        except Exception:\n",
        "            # sometimes the agent will return a string that is not a valid JSON\n",
        "            # often this happens when the agent is finished\n",
        "            # so we just return the text as the output\n",
        "            return AgentFinish({\"output\": text}, text)\n",
        "\n",
        "    @property\n",
        "    def _type(self) -> str:\n",
        "        return \"conversational_chat\"\n",
        "\n",
        "# initialize output parser for agent\n",
        "parser = OutputParser()\n",
        "\n",
        "template_tool_response = \"\"\"TOOL RESPONSE:\n",
        "{observation}\n",
        "USER: Okay, was ist also die Antwort auf meinen letzten Kommentar? Wenn du Informationen aus den Tool verwendest, musst du diese explizit erwähnen, ohne die Tool-Namen zu nennen - ich habe alle TOOL-RESPONSES vergessen! Denken Sie daran, mit einem Markdown-Code-Schnipsel eines json-Blob mit einer einzigen Aktion zu antworten, und sonst NICHTS.\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f9d3e15-e4da-42ef-9b19-4176959d2ed1",
      "metadata": {
        "id": "1f9d3e15-e4da-42ef-9b19-4176959d2ed1"
      },
      "source": [
        "Jetzt können wir den Agent initialisieren und gleich das `Memory` für das Gespräch, sowie alle andere verdefinierten Komponenten mit reingeben."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddcd2b4b",
      "metadata": {
        "tags": [],
        "id": "ddcd2b4b"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import initialize_agent\n",
        "\n",
        "# initialize agent\n",
        "agent = initialize_agent(\n",
        "    agent=\"chat-conversational-react-description\",\n",
        "    tools=tools,\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        "    early_stopping_method=\"generate\",\n",
        "    memory=memory,\n",
        "    handle_parsing_errors=True,\n",
        "    return_intermediate_steps=True,\n",
        "    agent_kwargs={\n",
        "        \"output_parser\": parser,\n",
        "        \"template_tool_response\": template_tool_response\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ac0c312",
      "metadata": {
        "tags": [],
        "id": "7ac0c312",
        "outputId": "b0fa3d87-b233-4e2f-98fd-fb4f8caa09a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Assistant is a large language model trained by OpenAI.\n",
            "\n",
            "Assistant is designed to be able to assist with a wide range of tasks, from answering simple questions to providing in-depth explanations and discussions on a wide range of topics. As a language model, Assistant is able to generate human-like text based on the input it receives, allowing it to engage in natural-sounding conversations and provide responses that are coherent and relevant to the topic at hand.\n",
            "\n",
            "Assistant is constantly learning and improving, and its capabilities are constantly evolving. It is able to process and understand large amounts of text, and can use this knowledge to provide accurate and informative responses to a wide range of questions. Additionally, Assistant is able to generate its own text based on the input it receives, allowing it to engage in discussions and provide explanations and descriptions on a wide range of topics.\n",
            "\n",
            "Overall, Assistant is a powerful system that can help with a wide range of tasks and provide valuable insights and information on a wide range of topics. Whether you need help with a specific question or just want to have a conversation about a particular topic, Assistant is here to assist.\n"
          ]
        }
      ],
      "source": [
        "print(agent.agent.llm_chain.prompt.messages[0].prompt.template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d190acf7-fb93-4dd4-bf1f-30ba17781cff",
      "metadata": {
        "tags": [],
        "id": "d190acf7-fb93-4dd4-bf1f-30ba17781cff"
      },
      "source": [
        "## Anpassen der Agent Prompts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94c1a5ac-2fd2-46d9-b78a-f9603a96a655",
      "metadata": {
        "id": "94c1a5ac-2fd2-46d9-b78a-f9603a96a655"
      },
      "source": [
        "In Langchain sind vordefinierte Prompts immer in Englisch. Wir müssen diese an unser LLM und an unsere Sprache anpassen. Hier sehen wir auch die zuvor erläuterte Gesprächsform, wo das Ergebnis das Tools als User Nachricht mit in des Gespräch geht. Wir geben dem `LLM` ein ausfürliches Beispiel, damit es den Syntax versteht."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1dfe2fd",
      "metadata": {
        "tags": [],
        "id": "e1dfe2fd"
      },
      "outputs": [],
      "source": [
        "agent_prompt_template = \"\"\"Du bist ein Experte im Erstellen von JSONs und entworfen, um bei einer Vielzahl von Aufgaben zu helfen.\n",
        "Du kannst dem Benutzer antworten und Tools mit JSON-Strings verwenden, die die Parameter \"action\" und \"action_input\" enthalten.\n",
        "Dein Gesamter Kommunikationsverlauf erfolgt in diesem JSON-Format. Bei einer reinen Textantwort ohne Tool, antworte mit der \"action\": \"Final Answer\".\n",
        "Du kannst auch Tools verwenden, indem du dem Benutzer Anweisungen zur Verwendung des Tools im gleichen \"action\" und \"action_input\" JSON-Format gibt. Für den Assistenten verfügbare Tools sind:\n",
        "\n",
        "- \"Rechner\": Nützlich, wenn Sie Fragen zur Mathematik beantworten müssen.\n",
        "  - Um das Rechner-Tool zu verwenden, sollte der Assistent so schreiben:\n",
        "    ```json\n",
        "    {{\"action\": \"Rechner\",\n",
        "      \"action_input\": \"sqrt(4)\"}}\n",
        "    ```\n",
        "Hier sind einige frühere Gespräche zwischen dem Assistenten und dem Benutzer:\n",
        "\n",
        "USER: Hey, wie geht es dir heute?\n",
        "ASSISTANT: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"Mir geht's gut, danke. Und dir?\"}}\n",
        "```\n",
        "USER: Mir geht's super, was ist die Quadratwurzel von 4?\n",
        "ASSISTANT: ```json\n",
        "{{\"action\": \"Rechner\",\n",
        "\"action_input\": \"sqrt(4)\"}}\n",
        "```\n",
        "USER: 2.0\n",
        "ASSISTANT: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"Es sieht so aus, als wäre die Antwort 2!\"}}\n",
        "```\n",
        "USER: Danke, kannst du mir sagen, was 4 hoch 2 ist?\n",
        "ASSISTANT: ```json\n",
        "{{\"action\": \"Rechner\",\n",
        "\"action_input\": \"4**2\"}}\n",
        "```\n",
        "USER: 16.0\n",
        "ASSISTANT: ```json\n",
        "{{\"action\": \"Final Answer\",\n",
        " \"action_input\": \"Es sieht so aus, als wäre die Antwort 16!\"}}\n",
        " ```\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b3826f6",
      "metadata": {
        "tags": [],
        "id": "7b3826f6",
        "outputId": "c96bd764-ba97-4530-805d-8be965593c36"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Du bist ein Experte im Erstellen von JSONs und entworfen, um bei einer Vielzahl von Aufgaben zu helfen.\n",
            "Du kannst dem Benutzer antworten und Tools mit JSON-Strings verwenden, die die Parameter \"action\" und \"action_input\" enthalten.\n",
            "Dein Gesamter Kommunikationsverlauf erfolgt in diesem JSON-Format. Bei einer reinen Textantwort ohne Tool, antworte mit der \"action\": \"Final Answer\".\n",
            "Du kannst auch Tools verwenden, indem du dem Benutzer Anweisungen zur Verwendung des Tools im gleichen \"action\" und \"action_input\" JSON-Format gibt. Für den Assistenten verfügbare Tools sind:\n",
            "\n",
            "- \"Rechner\": Nützlich, wenn Sie Fragen zur Mathematik beantworten müssen.\n",
            "  - Um das Rechner-Tool zu verwenden, sollte der Assistent so schreiben:\n",
            "    ```json\n",
            "    {{\"action\": \"Rechner\",\n",
            "      \"action_input\": \"sqrt(4)\"}}\n",
            "    ```\n",
            "Hier sind einige frühere Gespräche zwischen dem Assistenten und dem Benutzer:\n",
            "\n",
            "USER: Hey, wie geht es dir heute?\n",
            "ASSISTANT: ```json\n",
            "{{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"Mir geht's gut, danke. Und dir?\"}}\n",
            "```\n",
            "USER: Mir geht's super, was ist die Quadratwurzel von 4?\n",
            "ASSISTANT: ```json\n",
            "{{\"action\": \"Rechner\",\n",
            "\"action_input\": \"sqrt(4)\"}}\n",
            "```\n",
            "USER: 2.0\n",
            "ASSISTANT: ```json\n",
            "{{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"Es sieht so aus, als wäre die Antwort 2!\"}}\n",
            "```\n",
            "USER: Danke, kannst du mir sagen, was 4 hoch 2 ist?\n",
            "ASSISTANT: ```json\n",
            "{{\"action\": \"Rechner\",\n",
            "\"action_input\": \"4**2\"}}\n",
            "```\n",
            "USER: 16.0\n",
            "ASSISTANT: ```json\n",
            "{{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"Es sieht so aus, als wäre die Antwort 16!\"}}\n",
            " ```\n"
          ]
        }
      ],
      "source": [
        "print(agent_prompt_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "417b09d9-b0f7-4ad7-a250-9e83e065f5fe",
      "metadata": {
        "id": "417b09d9-b0f7-4ad7-a250-9e83e065f5fe"
      },
      "source": [
        "Jetzt bauen wir die Formatierungsanweisungen mit in unsere Prompt für den `Agent` ein."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ace8d15f",
      "metadata": {
        "tags": [],
        "id": "ace8d15f"
      },
      "outputs": [],
      "source": [
        "instruction = \"\"\"BEGININSTRUCTION TOOLS\n",
        "------\n",
        "Der Assistent kann den Benutzer auffordern, Tools zu verwenden, um Informationen nachzuschlagen, die bei der Beantwortung der ursprünglichen Frage des Benutzers hilfreich sein können. Die Tools, die der Benutzer verwenden kann, sind:\n",
        "\n",
        "{{tools}}\n",
        "\n",
        "{format_instructions}\n",
        "\n",
        "Denke daran, mit einem Markdown-Code-Schnipsel eines json-Blob mit einer einzigen Aktion zu antworten, und sonst NICHTS.\n",
        "ENDINSTRUCTION\"\"\"\n",
        "human_msg = instruction + \"USER: {{{{input}}}} ASSISTANT:\"\n",
        "\n",
        "new_prompt = agent.agent.create_prompt(\n",
        "    system_message=agent_prompt_template,\n",
        "    human_message=human_msg,\n",
        "    tools=tools,\n",
        "    output_parser= parser\n",
        ")\n",
        "agent.agent.llm_chain.prompt = new_prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b67a0c6d-9ff2-447c-9690-2103f2bb9e52",
      "metadata": {
        "id": "b67a0c6d-9ff2-447c-9690-2103f2bb9e52"
      },
      "source": [
        "## Run Agent"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da43ba1b-42b3-47b9-92eb-20e99905b413",
      "metadata": {
        "id": "da43ba1b-42b3-47b9-92eb-20e99905b413"
      },
      "source": [
        "Wenn wir die `Agent`-Chain ausführen, dann sollte dieser nur ein tool aufrufen, wenn wir eine mathematische Frage stellen. Deshalb starten wir als erstes mit einer normalen Frage und schauen ob er uns direkt eine Finale Antwort gibt. Diese sollte trotzdem als Json ausgegeben werden."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64d7a04e",
      "metadata": {
        "tags": [],
        "id": "64d7a04e",
        "outputId": "660ec30f-2732-4c79-bf0e-a740a77e02d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            " ```json\n",
            "{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"Mir geht's gut, danke. Und dir?\"}\n",
            "```\u001b[32;1m\u001b[1;3m ```json\n",
            "{\"action\": \"Final Answer\",\n",
            " \"action_input\": \"Mir geht's gut, danke. Und dir?\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'input': 'Hey, wie geht es dir heute?',\n",
              " 'chat_history': [HumanMessage(content='Hey, wie geht es dir heute?'),\n",
              "  AIMessage(content=\"Mir geht's gut, danke. Und dir?\")],\n",
              " 'output': \"Mir geht's gut, danke. Und dir?\",\n",
              " 'intermediate_steps': []}"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent(\"Hey, wie geht es dir heute?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940f55fc-7c9e-45b7-8591-509f56eef5b6",
      "metadata": {
        "id": "940f55fc-7c9e-45b7-8591-509f56eef5b6"
      },
      "source": [
        "Perfekt, der `Agent` hat kein Tool verwendet aber die richtige Formatierung verwendet. Lasst uns jetzt eine mathematische Frage stellen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38ed661b",
      "metadata": {
        "tags": [],
        "id": "38ed661b",
        "outputId": "01ec170e-930f-4991-921d-6dfea44d1a1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
            " ```json\n",
            "{\"action\": \"Rechner\",\n",
            "\"action_input\": \"4**2.1\"}\n",
            "```\u001b[32;1m\u001b[1;3m ```json\n",
            "{\"action\": \"Rechner\",\n",
            "\"action_input\": \"4**2.1\"}\n",
            "```\u001b[0m\n",
            "\n",
            "\u001b[1m> Entering new LLMMathChain chain...\u001b[0m\n",
            "4**2.1\n",
            "```text\n",
            "4**2.1\n",
            "```\n",
            "...numexpr.evaluate(\"4**2.1\")...\n",
            "```output\u001b[32;1m\u001b[1;3m\n",
            "```text\n",
            "4**2.1\n",
            "```\n",
            "...numexpr.evaluate(\"4**2.1\")...\n",
            "```output\u001b[0m\n",
            "Answer: \u001b[33;1m\u001b[1;3m18.37917367995256\u001b[0m\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "\n",
            "Observation: \u001b[36;1m\u001b[1;3mAnswer: 18.37917367995256\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "outputs = agent(\"Was ist 4 hoch 2.1?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fdda459b-50cb-44e4-a382-d9c421ced7f7",
      "metadata": {
        "tags": [],
        "id": "fdda459b-50cb-44e4-a382-d9c421ced7f7",
        "outputId": "61a2ffef-ee69-4373-e16d-9176567a798c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input': 'Was ist 4 hoch 2.1?',\n",
              " 'chat_history': [HumanMessage(content='Hey, wie geht es dir heute?'),\n",
              "  AIMessage(content=\"Mir geht's gut, danke. Und dir?\"),\n",
              "  HumanMessage(content='Was ist 4 hoch 2.1?'),\n",
              "  AIMessage(content='')],\n",
              " 'output': '',\n",
              " 'intermediate_steps': [(AgentAction(tool='Rechner', tool_input='4**2.1', log=' ```json\\n{\"action\": \"Rechner\",\\n\"action_input\": \"4**2.1\"}\\n```'),\n",
              "   'Answer: 18.37917367995256')]}"
            ]
          },
          "execution_count": 78,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74056780-5e76-4bbb-979c-f10e041b0e65",
      "metadata": {
        "tags": [],
        "id": "74056780-5e76-4bbb-979c-f10e041b0e65",
        "outputId": "d0f5f814-6349-4225-cac5-64f4c4d449e5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[AIMessage(content=' ```json\\n{\"action\": \"Rechner\",\\n\"action_input\": \"4**2.1\"}\\n```'),\n",
              " HumanMessage(content='TOOL RESPONSE:\\nAnswer: 18.37917367995256\\nUSER: Okay, was ist also die Antwort auf meinen letzten Kommentar? Wenn du Informationen aus den Tool verwendest, musst du diese explizit erwähnen, ohne die Tool-Namen zu nennen - ich habe alle TOOL-RESPONSES vergessen! Denken Sie daran, mit einem Markdown-Code-Schnipsel eines json-Blob mit einer einzigen Aktion zu antworten, und sonst NICHTS.')]"
            ]
          },
          "execution_count": 464,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent.agent._construct_scratchpad(outputs[\"intermediate_steps\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb37ee69-1671-4182-bbee-0c3189792dcd",
      "metadata": {
        "tags": [],
        "id": "bb37ee69-1671-4182-bbee-0c3189792dcd",
        "outputId": "8b49317e-3008-4a74-9145-9cb1f1d3d874"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "**Eingabe:**\n",
              "\n",
              "Was ist 4 hoch 2.1?\n",
              "\n",
              "**Chatverlauf:**\n",
              "\n",
              "- **Benutzer**: Hey, wie geht es dir heute?\n",
              "- **Assistent**: Mir geht's gut, danke. Und dir?\n",
              "- **Benutzer**: Was ist 4 hoch 2.1?\n",
              "- **Assistent**: \n",
              "\n",
              "### Ausgabe:\n",
              "\n",
              "\n",
              "\n",
              "**Zwischenschritte:**\n",
              "\n",
              "- **Aktion**: Rechner\n",
              "  - **Eingabe**: 4**2.1\n",
              "  - **Log**:\n",
              "\n",
              " ```json\n",
              "{\"action\": \"Rechner\",\n",
              "\"action_input\": \"4**2.1\"}\n",
              "```\n",
              "  - **Antwort**:  18.37917367995256\n",
              "\n"
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from IPython.display import display, Markdown\n",
        "from langchain.schema.messages import HumanMessage, AIMessage\n",
        "\n",
        "\n",
        "def format_output_for_jupyter(data):\n",
        "    markdown_output = \"\"\n",
        "\n",
        "    # Input section\n",
        "    markdown_output += f\"**Eingabe:**\\n\\n{data['input']}\\n\\n\"\n",
        "\n",
        "    # Chat history section\n",
        "    markdown_output += \"**Chatverlauf:**\\n\\n\"\n",
        "    for message in data['chat_history']:\n",
        "        if isinstance(message, HumanMessage):\n",
        "            markdown_output += f\"- **Benutzer**: {message.content}\\n\"\n",
        "        elif isinstance(message, AIMessage):\n",
        "            markdown_output += f\"- **Assistent**: {message.content}\\n\"\n",
        "    markdown_output += \"\\n\"\n",
        "\n",
        "    # Output section\n",
        "    markdown_output += f\"### Ausgabe:\\n\\n{data['output']}\\n\\n\"\n",
        "\n",
        "    # Intermediate steps section\n",
        "    markdown_output += \"**Zwischenschritte:**\\n\\n\"\n",
        "    for step in data['intermediate_steps']:\n",
        "        markdown_output += f\"- **Aktion**: {step[0].tool}\\n\"\n",
        "        markdown_output += f\"  - **Eingabe**: {step[0].tool_input}\\n\"\n",
        "        markdown_output += f\"  - **Log**:\\n\\n{step[0].log}\\n\"\n",
        "        markdown_output += f\"  - **Antwort**: {step[1].replace('Answer:', '')}\\n\\n\"\n",
        "\n",
        "    return markdown_output\n",
        "\n",
        "markdown_output = format_output_for_jupyter(outputs)\n",
        "display(Markdown(markdown_output))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d34a07e-d7e9-4dd4-b992-302dd12ced51",
      "metadata": {
        "id": "6d34a07e-d7e9-4dd4-b992-302dd12ced51"
      },
      "source": [
        "Perfekt! Der `Agent` hat unseren Rechner verwendet um das Ergebnis zu berechnen und wir bekommen dieses als Antwort. Thought bleibt aktuell noch leer, hier müsste man noch weiteres Prompt Engenieering verwenden, sodass das `LLM` einen Denkprozess anstößt und die Ergebnisse in eine Antwort einbaut. Dies würde jedoch den Rahmen dieses Tutorials sprengen."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}